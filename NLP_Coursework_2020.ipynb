{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Coursework_2020_extra.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"88KbEPMAmbxF","outputId":"65fc654e-eb16-4f78-8e4d-ca1d1c9a3db7","executionInfo":{"status":"ok","timestamp":1590336905553,"user_tz":-180,"elapsed":18345,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TDd4BKiQmvSg","outputId":"7b3a3343-537c-43f2-a875-d20a42270cb3","executionInfo":{"status":"ok","timestamp":1590336931294,"user_tz":-180,"elapsed":44066,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["import os\n","import re\n","import string\n","import json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","import requests\n","from random import randint\n","%matplotlib inline\n","import warnings\n","from collections import Counter\n","warnings.filterwarnings(\"ignore\")\n","try:\n","    from inflection import singularize\n","except:\n","    !pip install inflection\n","    from inflection import singularize\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.util import ngrams\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import BaggingRegressor\n","from xgboost import XGBRegressor\n","from xgboost import plot_importance\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import make_scorer"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["Collecting inflection\n","  Downloading https://files.pythonhosted.org/packages/52/c1/36be286d85dbd76527fb613527222a795d7c071da195fa916e7bf3cb03cb/inflection-0.4.0-py2.py3-none-any.whl\n","Installing collected packages: inflection\n","Successfully installed inflection-0.4.0\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tTN-4EN_sFxU","colab_type":"code","colab":{}},"source":["# Global items\n","try:\n","    with open('/content/drive/My Drive/NLP/spell_check_dict.json') as f:\n","        spell_check_dict = json.load(f)\n","    spelling_dictionary_imported = True\n","except:\n","    spell_check_dict = dict()\n","    spelling_dictionary_imported = False\n","    \n","randomSeed = 5\n","\n","# create a metric as RMSE is not provided in the scoring parameter in cross-validation\n","def fmean_squared_error(ground_truth, predictions):\n","    fmean_squared_error = np.sqrt(mean_squared_error(ground_truth, predictions))\n","    return fmean_squared_error\n","RMSE = make_scorer(fmean_squared_error, greater_is_better=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZHnD-KvWnKka","scrolled":true,"colab":{}},"source":["class RelevanceScorePrediction:\n","    def __init__(self, settings):\n","        self.path_to_files = settings['path_to_files']        \n","        self.train_path = self.path_to_files + \"/train.csv\"\n","        self.test_path = self.path_to_files + \"/test.csv\"\n","        self.product_descriptions_path = self.path_to_files + \"/product_descriptions.csv\"\n","        self.attributes_path = self.path_to_files + \"/attributes.csv\"\n","        \n","        self.df_train = pd.read_csv(self.train_path, encoding='ISO-8859-1', header=0)\n","        self.df_test = pd.read_csv(self.test_path, encoding='ISO-8859-1', header=0)\n","        self.df_product_desc = pd.read_csv(self.product_descriptions_path, header=0)\n","        self.df_product_attr = pd.read_csv(self.attributes_path, header=0)\n","\n","    # Functions that reads the dataframes  \n","    def reads_and_merges(self):\n","        print('The shape of train is:', self.df_train.shape)\n","        print('The info of train is: ')\n","        print(self.df_train.info())\n","        self.len_train = self.df_train.shape[0]\n","        print('The len_train is: ')\n","        print(self.len_train)\n","        \n","        print('\\n The shape of test is:', self.df_test.shape)\n","        print('The info of test is: ')\n","        print(self.df_test.info())\n","        \n","        print('\\n The shape of product description is: ', self.df_product_desc.shape)\n","        print('The info of product description is: ')\n","        print(self.df_product_desc.info())\n","\n","        print('\\n The shape of product attributes is: \\n', self.df_product_attr.shape)\n","        print('The null values of product attributes before drop are: \\n', self.df_product_attr.isnull().sum())\n","\n","        self.df_product_attr = self.df_product_attr.dropna(how=\"all\")\n","        self.df_product_attr[\"product_uid\"] = self.df_product_attr[\"product_uid\"].astype(\"int64\")\n","        print('The null values of product attributes after drop are: \\n', self.df_product_attr.isnull().sum())\n","        \n","        # merge train with test\n","        self.df_train_test = pd.concat([self.df_train, self.df_test], axis=0, ignore_index=True)\n","        print('The shape of train-test dataframe is: ', self.df_train_test.shape)\n","        \n","        # For attributes table, we are only interested in brand names which could be included in search queries.\n","        self.df_product_brand = self.df_product_attr[self.df_product_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n","        print('The shape of product_brand is: ', self.df_product_brand.shape)\n","        \n","        # merge train_test with product attributes\n","        self.df_train_test_brands = pd.merge(self.df_train_test, self.df_product_brand, how='left', on='product_uid')\n","        print('The shape of train_test_brands is: ', self.df_train_test_brands.shape)\n","        print('The info of train_test_brands is: ')\n","        print(self.df_train_test_brands.info())\n","        self.df_train_test_brands[\"brand\"] = self.df_train_test_brands[\"brand\"].astype(str)\n","        self.df_train_test_brands.fillna(\"unknown\", inplace=True)\n","        print('The info of train_test_brands after filling null values is: ')\n","        print(self.df_train_test_brands.info())\n","        \n","        return self.df_train, self.df_test, self.df_product_desc, self.df_product_attr, self.df_train_test, self.df_train_test_brands\n","\n","    def preprocessing(self):\n","        self.df_train, self.df_test, self.df_product_desc, self.df_product_attr, self.df_train_test, self.df_train_test_brands = self.reads_and_merges()\n","        \n","        # Function to perform simole regex\n","        def simple_regex(text, regex1, sub1, regex2, sub2, regex3, sub3):\n","            text = re.sub(regex1, sub1, text) # add space after . if it doesn't exist.also avoid add space after the last .\n","            text = re.sub(regex2, sub2, text) # do this also for ;\n","            text = re.sub(regex3, sub3, text) # add space Capital words that are in the form textTex\n","            return text\n","\n","        # Perfom simple regex\n","        print('Perform text corrections with regex')\n","        start_regex = time.time()\n","        regex1 = r\"\\.(?!\\s)(?!$)\"\n","        sub1 = \". \"\n","        regex2 = r\"\\;(?!\\s)(?!$)\"\n","        sub2 = \"; \"\n","        regex3 = r\"([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))\"\n","        sub3 = r\"\\1 \"\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"product_description...\")\n","        print('initial: \\n', self.df_product_desc['product_description'][1])\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print('after regex: \\n', self.df_product_desc['product_description'][1])\n","        end_regex = time.time()\n","        print(\"Simple regex finished in \", (end_regex - start_regex), \"seconds\")\n","\n","        # Function to correct measurement units\n","        def measurement_unit_correction(text):\n","            \"\"\"\n","            This function preprocess a string. Corrects measurement units.\n","            Input: A string\n","            Output: A string with corrected measurements\n","            Example: text = measurement_unit_correction(text)\n","            \"\"\"\n","            if isinstance(text, str):\n","\n","                metric = text.lower()\n","\n","                metric = metric.replace(\"centimeters\",\" cm \")    \n","\n","                metric = metric.replace(\"millimeters\",\" mm \")\n","\n","                metric = metric.replace(\"'\",\" inches \") \n","                # metric = metric.replace(\"inches\",\" in. \") \n","                metric = metric.replace(\"inch\",\" inches \")\n","                metric = metric.replace(\"''\",\" feet \") \n","                # metric = metric.replace(\"feet\",\" feet \") \n","                # metric = metric.replace(\"foot\",\" ft. \") \n","                metric = metric.replace(\"sq ft\",\" sqft \") \n","                # metric = metric.replace(\"sqft \",\" sq.ft. \")\n","                metric = metric.replace(\"sq. ft\",\" sqft \") \n","                metric = metric.replace(\"sq ft.\",\" sqft \") \n","                metric = metric.replace(\"sq feet\",\" sqft \") \n","                metric = metric.replace(\"square feet\",\" sqft \") \n","\n","                metric = metric.replace(\"pounds\",\" lb \")\n","                metric = metric.replace(\"pound\",\" lb \") \n","                metric = metric.replace(\"lbs \",\" lb \") \n","                metric = metric.replace(\"lbs.\",\" lb \") \n","\n","                metric = metric.replace(\" x \",\" xby \")\n","                metric = metric.replace(\"*\",\" xby \")\n","                metric = metric.replace(\"by\",\" xby \")\n","                metric = metric.replace(\"x0\",\" xby 0 \")\n","                metric = metric.replace(\"x1\",\" xby 1 \")\n","                metric = metric.replace(\"x2\",\" xby 2 \")\n","                metric = metric.replace(\"x3\",\" xby 3 \")\n","                metric = metric.replace(\"x4\",\" xby 4 \")\n","                metric = metric.replace(\"x5\",\" xby 5 \")\n","                metric = metric.replace(\"x6\",\" xby 6 \")\n","                metric = metric.replace(\"x7\",\" xby 7 \")\n","                metric = metric.replace(\"x8\",\" xby 8 \")\n","                metric = metric.replace(\"x9\",\" xby 9 \")\n","                metric = metric.replace(\"0x\",\" 0 xby \")\n","                metric = metric.replace(\"1x\",\" 1 xby \")\n","                metric = metric.replace(\"2x\",\" 2 xby \")\n","                metric = metric.replace(\"3x\",\" 3 xby \")\n","                metric = metric.replace(\"4x\",\" 4 xby \")\n","                metric = metric.replace(\"5x\",\" 5 xby \")\n","                metric = metric.replace(\"6x\",\" 6 xby \")\n","                metric = metric.replace(\"7x\",\" 7 xby \")\n","                metric = metric.replace(\"8x\",\" 8 xby \")\n","                metric = metric.replace(\"9x\",\" 9 xby \")\n","            \n","                metric = metric.replace(\"gallon\",\" gal \")\n","                metric = metric.replace(\"gallons\",\" gal \") \n","\n","                metric = metric.replace(\"ounces\",\" oz \")\n","                metric = metric.replace(\"ounce\",\" oz \")\n","                \n","                metric = metric.replace(\"°\",\" deg \")\n","                metric = metric.replace(\"degrees\",\" deg \")\n","                metric = metric.replace(\"degree\",\" deg \")\n","                \n","                metric = metric.replace(\"volts\",\" volt \")\n","\n","                metric = metric.replace(\"watts\",\" watt \")\n","                metric = metric.replace(\"watt\",\" watt \")\n","\n","                metric = metric.replace(\"ampere\",\" amp \")\n","                metric = metric.replace(\"amps\",\" amp \")\n","\n","                return metric\n","\n","        print('Measurement unit correction...')\n","        start_units = time.time()\n","        print('search term...')\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: measurement_unit_correction(x))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: measurement_unit_correction(x))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: measurement_unit_correction(x))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: measurement_unit_correction(x))\n","        end_units = time.time()\n","        print(\"Measurement unit correction finished in \", (end_units - start_units), \"seconds\")\n","\n","        # Function to remove stopwords from a string\n","        def stopwords_removal(text, cachedStopWords):\n","            \"\"\"\n","            This function preprocess a string. Removes stopwords.\n","            Input: A string\n","            Output: A string cleaned from stop_words\n","            Example: text = stopwords_removal(text)\n","            \"\"\"\n","            if not isinstance(text, str):\n","                return text\n","\n","            words = nltk.word_tokenize(text.lower())\n","            no_stopwords = [word for word in words if word not in cachedStopWords]\n","            return ' '.join(no_stopwords)\n","    \n","        # stopwords removal\n","        print(\"Removing stopwords...\")\n","        start_stopwords = time.time()\n","        cachedStopWords = stopwords.words('english')\n","        print('The stopwords are: ', cachedStopWords) # to see the english stopwords\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: stopwords_removal(x,cachedStopWords))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(self.df_product_desc['product_description'][1])\n","        end_stopwords = time.time()\n","        print(\"Stopwords removal finished in \", (end_stopwords - start_stopwords), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        # Function to remove punctuations from a string\n","        def punctuations_removal(text, translate_table_punctuations):\n","            \"\"\"\n","            This function preprocess a string. Removes punctuations.\n","            Input: A string\n","            Output: A string cleaned from punctuations\n","            Example: text = punctuation_removal(text)\n","            look also this: https://stackoverflow.com/questions/43935592/add-space-after-full-stops\n","            \"\"\"\n","            if not isinstance(text, str):\n","                return text\n","            return text.translate(translate_table_punctuations)\n","\n","        # punctuation removal\n","        print(\"Removing punctuations...\")\n","        start_punc = time.time()\n","        punctuations = string.punctuation\n","        print('The punctuations are: ', punctuations) # to see the punctuations\n","        translate_table_punctuations = str.maketrans(\"\",\"\",punctuations)\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(self.df_product_desc['product_description'][1])\n","        end_punc = time.time()\n","        print(\"Punctuation removal finished in \", (end_punc - start_punc), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        \n","        # Function to correct spelling errors\n","        def spell_check(text):\n","            if spelling_dictionary_imported:\n","                return spell_check_dict[text]\n","            \n","            def P(word, N=sum(WORDS.values())): \n","                \"Probability of `word`.\"\n","                return WORDS[word] / N\n","\n","            def correction(word): \n","                \"Most probable spelling correction for word.\"\n","                return max(candidates(word), key=P)\n","\n","            def candidates(word): \n","                \"Generate possible spelling corrections for word.\"\n","                return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","            def known(words): \n","                \"The subset of `words` that appear in the dictionary of WORDS.\"\n","                return set(w for w in words if w in WORDS)\n","\n","            def edits1(word):\n","                \"All edits that are one edit away from `word`.\"\n","                letters    = 'abcdefghijklmnopqrstuvwxyz'\n","                splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","                deletes    = [L + R[1:]               for L, R in splits if R]\n","                transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","                replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","                inserts    = [L + c + R               for L, R in splits for c in letters]\n","                return set(deletes + transposes + replaces + inserts)\n","\n","            def edits2(word): \n","                \"All edits that are two edits away from `word`.\"\n","                return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","\n","            lower_case_input = text.lower()\n","            tokens = word_tokenize(lower_case_input)\n","            corrected = [correction(token) for token in tokens]\n","            spell_check_dict[lower_case_input] = ' '.join(corrected)\n","            return ' '.join(corrected)\n","\n","        # Spell Correction\n","        if not spelling_dictionary_imported:\n","            print(\"Creating vocabulary for spell correction...\")\n","            wordsArr = []\n","            vocabulary_columns = [self.df_train_test_brands[\"product_title\"], self.df_product_desc[\"product_description\"], self.df_train_test_brands[\"brand\"]]\n","            for column in vocabulary_columns:\n","                for entry in column:\n","                    words = word_tokenize(entry.lower())\n","                    for word in words:\n","                        wordsArr.append(word)\n","\n","            WORDS = Counter(wordsArr)\n","            print(\"Top 5 most common words found in description, title and brand: \", WORDS.most_common(5))\n","        \n","        print(\"Spell correcting...\")\n","        start_spell = time.time()\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: spell_check(x))\n","        end_spell = time.time()\n","        print(self.df_product_desc['product_description'][1])\n","        print(\"Spell correction finished in \", (end_spell - start_spell), \"seconds\")\n","\n","        # Store the dictionary to file to use as static from now on\n","        if not spelling_dictionary_imported:\n","            with open('/content/drive/My Drive/spell_check_dict.json', 'w') as j:\n","                j.write(json.dumps(spell_check_dict))\n","        print(\"----------------------------------------------------\")\n","\n","        # Function to perform stemming\n","        def simple_stemming(text, snowball_stemmer):\n","            if not isinstance(text, str):\n","                return text\n","            tokens = word_tokenize(text.lower())\n","            stemmed = [snowball_stemmer.stem(word) for word in tokens]\n","            return ' '.join(stemmed)\n","\n","        # Perform simple stemming\n","        print(\"Performing simple stemming...\")\n","        start_stem = time.time()\n","        snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(self.df_product_desc['product_description'][1])\n","        end_stem = time.time()\n","        print(\"Simple stemming finished in \", (end_stem - start_stem), \"seconds\")\n","        \n","        self.df_merged = self.df_train_test_brands.merge(self.df_product_desc, how='left', on='product_uid')\n","        print('\\n The shape of train_test with products_description is: ', self.df_merged.shape)\n","        print(self.df_merged.info())\n","        return self.df_merged\n","    \n","    # Function to constract new features\n","    def feature_engineering(self):\n","        self.df_merged = self.preprocessing()\n","        \n","        # Add new columns with the length of the column\n","        print(\"creating len columns...\")\n","        start_length = time.time()\n","        self.df_merged['len_of_query'] = self.df_merged['search_term'].map(lambda x: len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_title'] = self.df_merged['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_description'] = self.df_merged['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_brand'] = self.df_merged['brand'].apply(lambda x:len(x.split())).astype(np.int64)\n","        end_length = time.time()\n","        print(\"Length calculation finished in \", (end_length - start_length), \"seconds\")\n","\n","        # Add a product_all_info column summarizing the attributes of products seperating by TAB\n","        print(\"creating combination of columns...\")\n","        self.df_merged['product_all_information'] = self.df_merged['search_term'] + \"\\t\" + self.df_merged['product_title'] + \"\\t\" + self.df_merged['product_description']\n","        self.df_merged['brand_helper'] = self.df_merged['search_term'] + \"\\t\" + self.df_merged['brand'] + \"\\t\" + self.df_merged['product_title']\n","\n","        # Function to find number of common words.\n","        def num_common_words(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","            return sum(int(str_2.find(word)>=0) for word in str_1.split())\n","\n","        # Add new columns with common words of the search term with product title and the search term with description\n","        print(\"creating common_words title/description...\")\n","        start_common_words = time.time()\n","        self.df_merged['common_words_in_title'] = self.df_merged['product_all_information'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[1]))\n","        self.df_merged['common_words_in_description'] = self.df_merged['product_all_information'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[2]))\n","        self.df_merged['common_words_in_brand'] = self.df_merged['brand_helper'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[1]))\n","        end_common_words = time.time()\n","        print(\"Common words calculation finished in \", (end_common_words - start_common_words), \"seconds\")\n","\n","        # Function to find common phrase\n","        def num_whole_word(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","\n","            str_1, str_2 = ''.join(str_1.lower().strip().split()), ''.join(str_2.lower().strip().split())\n","            l = len(str_1)\n","            ct = 0\n","            for c in range(0,len(str_2)):\n","                if str_2[c:c+l] == str_1:\n","                    ct += 1\n","            return ct\n","\n","        # Add new columns with number of times the entire search term appears in product title and number of times the entire search term appears in product description.\n","        print(\"creating query_in_X columns...\")\n","        start_whole_word = time.time()\n","        print(\"query_in_title...\")\n","        self.df_merged['query_in_title'] = self.df_merged['product_all_information'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[1]))\n","        print(\"query_in_description...\")\n","        self.df_merged['query_in_description'] = self.df_merged['product_all_information'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[2]))\n","        print(\"query_in_brand...\")\n","        self.df_merged['query_in_brand'] = self.df_merged['brand_helper'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[1]))\n","        end_whole_word = time.time()\n","        print(\"Whole word calculation finished in \", (end_whole_word - start_whole_word), \"seconds\")\n","\n","        # Add new columns with the ratio of common words in title with respect to query and ratio of common words in description with respect to query\n","        print(\"creating ratio columns...\")\n","        start_ratio = time.time()\n","        self.df_merged['ratio_title'] = self.df_merged['common_words_in_title'] / self.df_merged['len_of_query']\n","        self.df_merged['ratio_description'] = self.df_merged['common_words_in_description'] / self.df_merged['len_of_query']\n","        self.df_merged['ratio_brand'] = self.df_merged['query_in_brand'] / self.df_merged['len_of_brand']\n","\n","        self.df_merged['query_title_len_prop'] = self.df_merged['len_of_title'] / self.df_merged['len_of_query']\n","        self.df_merged['query_desc_len_prop'] = self.df_merged['len_of_description'] / self.df_merged['len_of_query']\n","        self.df_merged = self.df_merged.replace([np.inf, -np.inf], np.nan)\n","        print(self.df_merged.info())\n","        self.df_merged['ratio_title'].fillna(0.0, inplace = True)\n","        self.df_merged['ratio_description'].fillna(0.0, inplace = True)\n","        self.df_merged['query_title_len_prop'].fillna(0.0, inplace = True)\n","        self.df_merged['query_desc_len_prop'].fillna(0.0, inplace = True)\n","        print(self.df_merged.info())\n","        end_ratio = time.time()\n","        print(\"Ratio calculation finished in \", (end_ratio - start_ratio), \"seconds\")\n","        \n","      # Function to compute jaccard similarity\n","        def jaccard_sim(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","            set_1 = set(str_1.split())\n","            set_2 = set(str_2.split())\n","            if len(set_1.union(set_2)) == 0:\n","              return 0\n","            return len(set_1.intersection(set_2)) / (float(len(set_1.union(set_2))))\n","\n","        # Add new columns with jaccard similarity of search term and product title and search term and product description\n","        print(\"creating jacard columns ....\")\n","        start_jaccard = time.time()\n","        self.df_merged['jaccard_search_and_title'] = self.df_merged['product_all_information'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[1])).round(3)\n","        self.df_merged['jaccard_search_and_description'] = self.df_merged['product_all_information'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[2])).round(3)\n","        self.df_merged['jaccard_search_and_brand'] = self.df_merged['brand_helper'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[1])).round(3)\n","        end_jaccard = time.time()\n","        print(\"Jaccard calculation finished in \", (end_jaccard - start_jaccard), \"seconds\")\n","\n","        # Function to compute the number of common ngrams\n","        def common_ngrams(str_1, str_2, n):\n","            bigrams_1 = ngrams(str_1.lower().split(), n)\n","            bigrams_2 = ngrams(str_2.lower().split(), n)\n","            common = []\n","            for grams_1 in bigrams_1:\n","                if grams_1 in bigrams_2:\n","                    common.append(grams_1)\n","            if not common:\n","              return 0\n","            else:\n","              return len(common)\n","\n","        # Add new columns with number of common ngrams\n","        print(\"creating ngrams columns ....\")\n","        start_ngrams = time.time()\n","        self.df_merged['ngrams_search_and_title'] = self.df_merged['product_all_information'].map(lambda x: common_ngrams(x.split('\\t')[0], x.split('\\t')[1], 2))\n","        self.df_merged['ngrams_search_and_description'] = self.df_merged['product_all_information'].map(lambda x: common_ngrams(x.split('\\t')[0], x.split('\\t')[2], 2))\n","        self.df_merged['ngrams_search_and_brand'] = self.df_merged['brand_helper'].map(lambda x: common_ngrams(x.split('\\t')[0], x.split('\\t')[2], 2))\n","        end_ngrams = time.time()\n","        print(\"ngrams calculation finished in \", (end_ngrams - start_ngrams), \"seconds\")\n","\n","        return self.df_merged\n","    \n","\n","    def train_and_predict(self, compare_many=False, grid=False, model=True):\n","        self.df_merged = self.feature_engineering()\n","        print(self.df_merged.head(2))\n","        # Drop the unecessary columns\n","        df = self.df_merged.drop(columns=['search_term','product_title','product_description','product_all_information','product_uid','brand','brand_helper'],axis=1)\n","        print(df.info())\n","\n","        df_train = df.iloc[:self.len_train]\n","        # df_train.reset_index(drop=True, inplace=True)\n","        print(df_train.shape)\n","        print(df_train.head(5))\n","\n","        df_test = df.iloc[self.len_train:]\n","        # df_test.reset_index(drop=True, inplace=True)\n","        print(df_test.shape)\n","        id_test = df_test['id'] # keep this for exporting to csv\n","        print(id_test.shape)\n","        print(df_test.head(5))\n","\n","        X = df_train.drop(columns=['relevance', 'id']) # drop label and id\n","        y = df_train['relevance']\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                            y,\n","                                                            train_size=0.7,\n","                                                            test_size=0.3,\n","                                                            random_state=randomSeed,\n","                                                            )\n","        print('X_train shape ', X_train.shape)\n","        print('X test shape ', X_test.shape)\n","\n","        if compare_many:\n","            dict_regressors = {\"Linear\": LinearRegression(),\n","                              \"Ridge\": Ridge(),\n","                              \"SVR\": SVR(),\n","                              \"RF\": RandomForestRegressor(),\n","                              \"GBR\": GradientBoostingRegressor(),\n","                              \"XGBoost\": XGBRegressor(),\n","                              }\n","\n","            for name, model in dict_regressors.items():\n","                model.fit(X_train, y_train)\n","                y_pred = model.predict(X_test)\n","                mse = mean_squared_error(y_pred, y_test)\n","                rmse = np.sqrt(mse)\n","                print(name, ' RMSE is: %.4f' % rmse)\n","\n","        elif grid:\n","            # gb = GradientBoostingRegressor(random_state=randomSeed)\n","\n","            # # Set the parameter values for GridSearch\n","            # param_grid = {'n_estimators': [100, 200, 400, 600],\n","            #               'min_samples_leaf':[3, 5, 7],\n","            #               'max_depth': [2, 4, 6]\n","            #              }\n","            # model = GridSearchCV(estimator= gb,\n","            #                     param_grid= param_grid,\n","            #                     n_jobs= -1,\n","            #                     cv= 10, \n","            #                     verbose= 20,\n","            #                     scoring= RMSE)\n","\n","            # model.fit(X_train, y_train)\n","            # y_pred = model.predict(X_test)\n","            # print(\"GB: Best parameters found by grid search:\", model.best_params_)\n","            # print(\"Gradient Boosting's best CV score: %.4f\" % model.best_score_)\n","\n","            # print('-----------------------------------------------------------')\n","\n","            xgbr = XGBRegressor(random_state=randomSeed)\n","            parameters = {'min_child_weight': [3, 4, 6],\n","                          'max_depth': [6],\n","                          'subsample ': [0.7, 0.75, 0.8],\n","                          'colsample_bytree ': [0.7, 0.75, 0.8],\n","                          'learning_rate': [0.1, 0.2, 0.3]\n","                        }\n","\n","            xgbr_grid = GridSearchCV(estimator= xgbr,\n","                                     param_grid= parameters,\n","                                     n_jobs= -1,\n","                                     cv= 2, \n","                                     verbose= 20,\n","                                     scoring= RMSE                                     \n","                                    )\n","            \n","            xgbr_grid.fit(X_train, y_train)\n","            y_pred = xgbr_grid.predict(X_test)\n","\n","            mse = mean_squared_error(y_pred, y_test)\n","            rmse = np.sqrt(mse)\n","            print('RMSE is: %.4f' % rmse)\n","\n","            print(\"XGB: Best parameters found by grid search:\", xgbr_grid.best_params_)\n","            print(\"XGB's best CV score: %.4f\" % xgbr_grid.best_score_)\n","\n","            # XGB: Best parameters found by grid search: {'colsample_bytree ': 0.75, 'max_depth': 6, 'min_child_weight': 6, 'subsample ': 0.75}\n","            # XGB's best CV score: -0.4799\n","\n","            # XGB: Best parameters found by grid search: {'colsample_bytree ': 0.6, 'learning_rate': 0.1, 'max_depth': 5, \n","            # 'min_child_weight': 4, 'subsample ': 0.5}\n","            # XGB's best CV score: -0.4813\n","            \n","            # predictions = pd.DataFrame({\"id\": id_test,\n","            #                     \"relevance\": y_pred})\n","            # predictions.to_csv('predictions_rf.csv', index=False)\n","        \n","        elif model:\n","            xgbr_tunned = XGBRegressor(\n","                random_state=randomSeed,\n","                n_estimators=1000,\n","                learning_rate=0.1,\n","                colsample_bytree=0.6,\n","                max_depth=5,\n","                min_child_weight=4,\n","                subsample=0.5)\n","\n","            xgbr_tunned.fit(X_train, y_train)\n","            y_pred = xgbr_tunned.predict(X_test)\n","\n","            mse = mean_squared_error(y_pred, y_test)\n","            rmse = np.sqrt(mse)\n","            print('RMSE is: %.4f' % rmse)\n","\n","            x_ax = range(len(y_test))\n","            plt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\n","            plt.plot(x_ax, y_pred, lw=0.8, color=\"red\", label=\"predicted\")\n","            plt.legend()\n","            plt.show()\n","\n","            feat_importances = pd.Series(xgbr_tunned.feature_importances_, index=X.columns)\n","            feat_importances = feat_importances.nlargest(19)\n","            feat_importances.plot(kind='barh', figsize=(10,10)) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqCtUyhbT2pp","colab_type":"code","colab":{}},"source":["settings = {\n","    'path_to_files': '/content/drive/My Drive/NLP', # path_to_files = \"/content/drive/My Drive/NLP\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDjhx5fGT2pv","colab_type":"code","colab":{}},"source":["rsp = RelevanceScorePrediction(settings)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"vIdOM6tHT2p0","colab_type":"code","outputId":"5b936e76-78e6-43d5-c160-7b8f3483a8ef","executionInfo":{"status":"ok","timestamp":1590267679940,"user_tz":-180,"elapsed":1182944,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# rsp.train_and_predict(compare_many=True)\n","rsp.train_and_predict(grid=True)\n","# rsp.train_and_predict(model=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The shape of train is: (74067, 5)\n","The info of train is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 74067 entries, 0 to 74066\n","Data columns (total 5 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   id             74067 non-null  int64  \n"," 1   product_uid    74067 non-null  int64  \n"," 2   product_title  74067 non-null  object \n"," 3   search_term    74067 non-null  object \n"," 4   relevance      74067 non-null  float64\n","dtypes: float64(1), int64(2), object(2)\n","memory usage: 2.8+ MB\n","None\n","The len_train is: \n","74067\n","\n"," The shape of test is: (166693, 4)\n","The info of test is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 166693 entries, 0 to 166692\n","Data columns (total 4 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             166693 non-null  int64 \n"," 1   product_uid    166693 non-null  int64 \n"," 2   product_title  166693 non-null  object\n"," 3   search_term    166693 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 5.1+ MB\n","None\n","\n"," The shape of product description is:  (124428, 2)\n","The info of product description is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 124428 entries, 0 to 124427\n","Data columns (total 2 columns):\n"," #   Column               Non-Null Count   Dtype \n","---  ------               --------------   ----- \n"," 0   product_uid          124428 non-null  int64 \n"," 1   product_description  124428 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 1.9+ MB\n","None\n","\n"," The shape of product attributes is: \n"," (2044803, 3)\n","The null values of product attributes before drop are: \n"," product_uid     155\n","name            155\n","value          2284\n","dtype: int64\n","The null values of product attributes after drop are: \n"," product_uid       0\n","name              0\n","value          2129\n","dtype: int64\n","The shape of train-test dataframe is:  (240760, 5)\n","The shape of product_brand is:  (86250, 2)\n","The shape of train_test_brands is:  (240760, 6)\n","The info of train_test_brands is: \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count   Dtype  \n","---  ------         --------------   -----  \n"," 0   id             240760 non-null  int64  \n"," 1   product_uid    240760 non-null  int64  \n"," 2   product_title  240760 non-null  object \n"," 3   search_term    240760 non-null  object \n"," 4   relevance      74067 non-null   float64\n"," 5   brand          194623 non-null  object \n","dtypes: float64(1), int64(2), object(3)\n","memory usage: 12.9+ MB\n","None\n","The info of train_test_brands after filling null values is: \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             240760 non-null  int64 \n"," 1   product_uid    240760 non-null  int64 \n"," 2   product_title  240760 non-null  object\n"," 3   search_term    240760 non-null  object\n"," 4   relevance      240760 non-null  object\n"," 5   brand          240760 non-null  object\n","dtypes: int64(2), object(4)\n","memory usage: 12.9+ MB\n","None\n","Perform text corrections with regex\n","product_title...\n","brand...\n","product_description...\n","initial: \n"," BEHR Premium Textured DECKOVER is an innovative solid color coating. It will bring your old, weathered wood or concrete back to life. The advanced 100% acrylic resin formula creates a durable coating for your tired and worn out deck, rejuvenating to a whole new look.  For the best results, be sure to properly prepare the surface using other applicable BEHR products displayed above.California residents: see&nbsp;Proposition 65 informationRevives wood and composite decks, railings, porches and boat docks, also great for concrete pool decks, patios and sidewalks100% acrylic solid color coatingResists cracking and peeling and conceals splinters and cracks up to 1/4 in.Provides a durable, mildew resistant finishCovers up to 75 sq. ft. in 2 coats per gallonCreates a textured, slip-resistant finishFor best results, prepare with the appropriate BEHR product for your wood or concrete surfaceActual paint colors may vary from on-screen and printer representationsColors available to be tinted in most storesOnline Price includes Paint Care fee in the following states: CA, CO, CT, ME, MN, OR, RI, VT\n","after regex: \n"," BEHR Premium Textured DECKOVER is an innovative solid color coating. It will bring your old, weathered wood or concrete back to life. The advanced 100% acrylic resin formula creates a durable coating for your tired and worn out deck, rejuvenating to a whole new look.  For the best results, be sure to properly prepare the surface using other applicable BEHR products displayed above. California residents: see&nbsp; Proposition 65 information Revives wood and composite decks, railings, porches and boat docks, also great for concrete pool decks, patios and sidewalks100% acrylic solid color coating Resists cracking and peeling and conceals splinters and cracks up to 1/4 in. Provides a durable, mildew resistant finish Covers up to 75 sq. ft. in 2 coats per gallon Creates a textured, slip-resistant finish For best results, prepare with the appropriate BEHR product for your wood or concrete surface Actual paint colors may vary from on-screen and printer representations Colors available to be tinted in most stores Online Price includes Paint Care fee in the following states: CA, CO, CT, ME, MN, OR, RI, VT\n","Simple regex finished in  11.067579746246338 seconds\n","Measurement unit correction...\n","search term...\n","product_title...\n","brand...\n","product_description...\n","Measurement unit correction finished in  10.932990312576294 seconds\n","Removing stopwords...\n","The stopwords are:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","search_term...\n","product_title...\n","brand...\n","product_description...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tvr0gBrd7w26","colab_type":"code","colab":{}},"source":["drive.flush_and_unmount()\n","print('All changes made in this colab session should now be visible in Drive.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZcI-sVyxtF8u","colab":{}},"source":["# rf = RandomForestRegressor(random_state=randomSeed)\n","\n","# # Set the parameter values for GridSearch\n","# param_grid = {'max_samples': [0.1, 0.2, 0.4],\n","#               'n_estimators': [100, 200, 300],\n","#               'min_samples_split': [2, 4, 6]\n","#              }\n","\n","# model = GridSearchCV(estimator= rf,\n","#                     param_grid= param_grid,\n","#                     n_jobs= -1,\n","#                     cv= 10, \n","#                     verbose= 20,\n","#                     scoring= RMSE\n","#                     )\n","\n","# model.fit(X_train, y_train)\n","# y_pred = model.predict(X_test)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)\n","\n","# # Gradient Boosting regressor. It has not been used in conjunction to Bagging regressor\n","# gb = GradientBoostingRegressor(random_state=randomSeed)\n","# gb.fit(X_train, y_train)\n","# y_pred = gb.predict(X_test)\n","\n","# model = GridSearchCV(estimator= clf,\n","#                                 param_grid= param_grid,\n","#                                 n_jobs= -1,\n","#                                 cv= 2, \n","#                                 verbose= 20,\n","#                                 scoring= RMSE)\n","# model.fit(X_train, y_train)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sNVwwJXbtbZG"},"source":["3. Bagging Regressor to improve Random Forest"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aud7y-wttePq","colab":{}},"source":["# clf = BaggingRegressor(base_estimator= rf, random_state=randomSeed)\n","# clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# # Set the parameter values for GridSearch\n","# param_grid = {'max_samples': [0.1, 0.2],\n","#               'n_estimators': [10, 100, 150],\n","#               'min_samples_split': [2, 4, 6]\n","#              }\n","\n","# model = GridSearchCV(\n","#     estimator= rf,\n","#     param_grid= param_grid,\n","#     n_jobs= -1,\n","#     cv= 10, \n","#     verbose= 20,\n","#     scoring= RMSE\n","# )\n","\n","# model.fit(X_train, y_train)\n","# y_pred = model.predict(X_test)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzNbcBbIqbV8","colab_type":"code","colab":{}},"source":["# descr = pd.read_csv('/content/drive/My Drive/Colab Notebooks/product_descriptions.csv', header=0)\n","# pd.options.display.max_rows = 100\n","# pd.set_option('max_colwidth', 9000)\n","# print(descr[descr['product_description'].str.contains(\"oz\")]['product_description'])\n","\n","# # descr['product_description'].apply(lambda x: ngrams(x.lower().split(), 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"am0UeNYZ-aM6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}