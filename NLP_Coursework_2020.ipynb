{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Coursework_2020.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"88KbEPMAmbxF","outputId":"74be7d79-aabd-4287-969a-632c0f391fa3","executionInfo":{"status":"ok","timestamp":1589731742533,"user_tz":-180,"elapsed":1065,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TDd4BKiQmvSg","outputId":"4bdcca75-2039-43c4-f5ea-2e4cd9c04b4c","executionInfo":{"status":"ok","timestamp":1589731743907,"user_tz":-180,"elapsed":2408,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["import os\n","import re\n","import string\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","import requests\n","from random import randint\n","%matplotlib inline\n","import warnings\n","from collections import Counter\n","warnings.filterwarnings(\"ignore\")\n","try:\n","    from autocorrect import Speller\n","except:\n","    !pip install autocorrect\n","    from autocorrect import Speller\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import BaggingRegressor\n","from xgboost import XGBRegressor\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import make_scorer"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZHnD-KvWnKka","scrolled":true,"colab":{}},"source":["class RelevanceScorePrediction:\n","    def __init__(self, settings):\n","        self.path_to_files = settings['path_to_files']        \n","        self.train_path = self.path_to_files + \"/train.csv\"\n","        self.test_path = self.path_to_files + \"/test.csv\"\n","        self.product_descriptions_path = self.path_to_files + \"/product_descriptions.csv\"\n","        self.attributes_path = self.path_to_files + \"/attributes.csv\"\n","        \n","        self.df_train = pd.read_csv(self.train_path, encoding='ISO-8859-1', header=0)\n","        self.df_test = pd.read_csv(self.test_path, encoding='ISO-8859-1', header=0)\n","        self.df_product_desc = pd.read_csv(self.product_descriptions_path, header=0)\n","        self.df_product_attr = pd.read_csv(self.attributes_path, header=0)\n","\n","    # Functions that reads the dataframes  \n","    def reads_and_merges(self):\n","        print('The shape of train is:', self.df_train.shape)\n","        print('The info of train is: ')\n","        print(self.df_train.info())\n","        self.len_train = self.df_train.shape[0]\n","        print('The len_train is: ')\n","        print(self.len_train)\n","        \n","        print('\\n The shape of test is:', self.df_test.shape)\n","        print('The info of test is: ')\n","        print(self.df_test.info())\n","        \n","        print('\\n The shape of product description is: ', self.df_product_desc.shape)\n","        print('The info of product description is: ')\n","        print(self.df_product_desc.info())\n","\n","        print('\\n The shape of product attributes is: \\n', self.df_product_attr.shape)\n","        print('The null values of product attributes before drop are: \\n', self.df_product_attr.isnull().sum())\n","\n","        self.df_product_attr = self.df_product_attr.dropna(how=\"all\")\n","        self.df_product_attr[\"product_uid\"] = self.df_product_attr[\"product_uid\"].astype(\"int64\")\n","        print('The null values of product attributes after drop are: \\n', self.df_product_attr.isnull().sum())\n","        \n","        # merge train with test\n","        self.df_train_test = pd.concat([self.df_train, self.df_test], axis=0, ignore_index=True)\n","        print('The shape of train-test dataframe is: ', self.df_train_test.shape)\n","        \n","        # For attributes table, we are only interested in brand names which could be included in search queries.\n","        self.df_product_brand = self.df_product_attr[self.df_product_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n","        print('The shape of product_brand is: ', self.df_product_brand.shape)\n","        \n","        # merge train_test with product attributes\n","        self.df_train_test_brands = pd.merge(self.df_train_test, self.df_product_brand, how='left', on='product_uid')\n","        print('The shape of train_test_brands is: ', self.df_train_test_brands.shape)\n","        print('The info of train_test_brands is: ')\n","        print(self.df_train_test_brands.info())\n","        self.df_train_test_brands[\"brand\"] = self.df_train_test_brands[\"brand\"].astype(str)\n","        self.df_train_test_brands.fillna(\"unknown\", inplace=True)\n","        print('The info of train_test_brands after filling null values is: ')\n","        print(self.df_train_test_brands.info())\n","        \n","        return self.df_train, self.df_test, self.df_product_desc, self.df_product_attr, self.df_train_test, self.df_train_test_brands\n","\n","    def preprocessing(self):\n","        self.df_train, self.df_test, self.df_product_desc, self.df_product_attr, self.df_train_test, self.df_train_test_brands = self.reads_and_merges()\n","\n","        spell = Speller(lang=\"en\")\n","        \n","        # Function to correct spelling errors\n","        def spell_check(text):\n","            # lower_text = text.lower()\n","            # return spell(lower_text)\n","            def P(word, N=sum(WORDS.values())): \n","                \"Probability of `word`.\"\n","                return WORDS[word] / N\n","\n","            def correction(word): \n","                \"Most probable spelling correction for word.\"\n","                return max(candidates(word), key=P)\n","\n","            def candidates(word): \n","                \"Generate possible spelling corrections for word.\"\n","                return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","            def known(words): \n","                \"The subset of `words` that appear in the dictionary of WORDS.\"\n","                return set(w for w in words if w in WORDS)\n","\n","            def edits1(word):\n","                \"All edits that are one edit away from `word`.\"\n","                letters    = 'abcdefghijklmnopqrstuvwxyz'\n","                splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","                deletes    = [L + R[1:]               for L, R in splits if R]\n","                transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","                replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","                inserts    = [L + c + R               for L, R in splits for c in letters]\n","                return set(deletes + transposes + replaces + inserts)\n","\n","            def edits2(word): \n","                \"All edits that are two edits away from `word`.\"\n","                return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","\n","            tokens = word_tokenize(text.lower())\n","            corrected = [correction(token) for token in tokens]\n","            # print(\"input: \", text.lower())\n","            # print(\"output: \", ' '.join(corrected))\n","            return ' '.join(corrected)\n","        \n","        # Function to perform simole regex\n","        def simple_regex(text, regex1, sub1, regex2, sub2, regex3, sub3):\n","            text = re.sub(regex1, sub1, text) # add space after . if it doesn't exist.also avoid add space after the last .\n","            text = re.sub(regex2, sub2, text) # do this also for ;\n","            text = re.sub(regex3, sub3, text) # add space Capital words that are in the form textTex\n","            return text\n","\n","        # Perfom simple regex\n","        print('Perform text corrections with regex')\n","        start_regex = time.time()\n","        regex1 = r\"\\.(?!\\s)(?!$)\"\n","        sub1 = \". \"\n","        regex2 = r\"\\;(?!\\s)(?!$)\"\n","        sub2 = \"; \"\n","        regex3 = r\"([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))\"\n","        sub3 = r\"\\1 \"\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(self.df_product_desc['product_description'][1])\n","        end_regex = time.time()\n","        print(\"Simple regex finished in \", (end_regex - start_regex), \"seconds\")\n","\n","        # Function to remove stopwords from a string\n","        def stopwords_removal(text, cachedStopWords):\n","            \"\"\"\n","            This function preprocess a string. Removes stopwords.\n","            Input: A string\n","            Output: A string cleaned from stop_words\n","            Example: text = stopwords_removal(text)\n","            \"\"\"\n","            if not isinstance(text, str):\n","                return text\n","\n","            words = nltk.word_tokenize(text.lower())\n","            no_stopwords = [word for word in words if word not in cachedStopWords]\n","            return ' '.join(no_stopwords)\n","    \n","        # stopwords removal\n","        print(\"Removing stopwords...\")\n","        start_stopwords = time.time()\n","        cachedStopWords = stopwords.words('english')\n","        print('The stopwords are: ', cachedStopWords) # to see the english stopwords\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: stopwords_removal(x,cachedStopWords))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(self.df_product_desc['product_description'][1])\n","        end_stopwords = time.time()\n","        print(\"Stopwords removal finished in \", (end_stopwords - start_stopwords), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        # Function to remove punctuations from a string\n","        def punctuations_removal(text, translate_table_punctuations):\n","            \"\"\"\n","            This function preprocess a string. Removes punctuations.\n","            Input: A string\n","            Output: A string cleaned from punctuations\n","            Example: text = punctuation_removal(text)\n","            look also this: https://stackoverflow.com/questions/43935592/add-space-after-full-stops\n","            \"\"\"\n","            if not isinstance(text, str):\n","                return text\n","            return text.translate(translate_table_punctuations)\n","\n","        # punctuation removal\n","        print(\"Removing punctuations...\")\n","        start_punc = time.time()\n","        punctuations = string.punctuation\n","        print('The punctuations are: ', punctuations) # to see the punctuations\n","        translate_table_punctuations = str.maketrans(\"\",\"\",punctuations)\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(self.df_product_desc['product_description'][1])\n","        end_punc = time.time()\n","        print(\"Punctuation removal finished in \", (end_punc - start_punc), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        # Spell Correction\n","        print(\"Creating vocabulary for spell correction...\")\n","        wordsArr = []\n","        vocabulary_columns = [self.df_train_test_brands[\"product_title\"], self.df_product_desc[\"product_description\"], self.df_train_test_brands[\"brand\"]]\n","        i = 1\n","        for column in vocabulary_columns:\n","            print(\"column \", i)\n","            for entry in column:\n","                words = word_tokenize(entry.lower())\n","                for word in words:\n","                    wordsArr.append(word)\n","            i = i + 1\n","\n","        WORDS = Counter(wordsArr)\n","        print(\"Top 5 most common words found in description, title and brand: \", WORDS.most_common(5))\n","\n","        print(\"Spell correcting...\")\n","        start_spell = time.time()\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: spell_check(x))\n","        end_spell = time.time()\n","        print(\"Spell correction finished in \", (end_spell - start_spell), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        # Function to perform stemming\n","        def simple_stemming(text, snowball_stemmer):\n","            if not isinstance(text, str):\n","                return text\n","\n","            tokens = word_tokenize(text.lower())\n","            stemmed = [snowball_stemmer.stem(word) for word in tokens]\n","            return ' '.join(stemmed)\n","\n","        # Perform simple stemming\n","        print(\"Performing simple stemming...\")\n","        start_stem = time.time()\n","        snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(self.df_product_desc['product_description'][1])\n","        end_stem = time.time()\n","        print(\"Simple stemming finished in \", (end_stem - start_stem), \"seconds\")\n","        \n","        self.df_merged = self.df_train_test_brands.merge(self.df_product_desc, how='left', on='product_uid')\n","        print('\\n The shape of train_test with products_description is: ', self.df_merged.shape)\n","        print(self.df_merged.info())\n","        return self.df_merged\n","    \n","    # Function to constract new features\n","    def feature_engineering(self):\n","        self.df_merged = self.preprocessing()\n","        \n","        # Add new columns with the length of the column\n","        print(\"creating len columns...\")\n","        start_length = time.time()\n","        self.df_merged['len_of_query'] = self.df_merged['search_term'].map(lambda x: len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_title'] = self.df_merged['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_description'] = self.df_merged['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_brand'] = self.df_merged['brand'].apply(lambda x:len(x.split())).astype(np.int64)\n","        end_length = time.time()\n","        print(\"Length calculation finished in \", (end_length - start_length), \"seconds\")\n","\n","        # Add a product_all_info column summarizing the attributes of products seperating by TAB\n","        print(\"creating combination of columns...\")\n","        self.df_merged['product_all_information'] = self.df_merged['search_term'] + \"\\t\" + self.df_merged['product_title'] + \"\\t\" + self.df_merged['product_description']\n","        self.df_merged['brand_helper'] = self.df_merged['search_term'] + \"\\t\" + self.df_merged['brand'] + \"\\t\" + self.df_merged['product_title']\n","\n","        # Function to find number of common words.\n","        def num_common_words(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","            return sum(int(str_2.find(word)>=0) for word in str_1.split())\n","\n","        # Add new columns with common words of the search term with product title and the search term with description\n","        print(\"creating common_words title/description...\")\n","        start_common_words = time.time()\n","        self.df_merged['common_words_in_title'] = self.df_merged['product_all_information'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[1]))\n","        self.df_merged['common_words_in_description'] = self.df_merged['product_all_information'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[2]))\n","        self.df_merged['common_words_in_brand'] = self.df_merged['brand_helper'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[1]))\n","        end_common_words = time.time()\n","        print(\"Common words calculation finished in \", (end_common_words - start_common_words), \"seconds\")\n","\n","        # Function to find common phrase\n","        def num_whole_word(str_1, str_2, i):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","\n","            str_1, str_2 = str_1.strip(), str_2.strip()\n","            count = 0\n","            while i < len(str_2):\n","                i = str_2.find(str_1, i)\n","                if i == -1:\n","                    return count\n","                else:\n","                    count += 1\n","                    i += len(str_1)\n","            return count\n","\n","        # Add new columns with number of times the entire search term appears in product title and number of times the entire search term appears in product description.\n","        print(\"creating query_in_X columns...\")\n","        start_whole_word = time.time()\n","        self.df_merged['query_in_title'] = self.df_merged['product_all_information'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[1],0))\n","        self.df_merged['query_in_description'] = self.df_merged['product_all_information'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[2],0))\n","        self.df_merged['query_in_brand'] = self.df_merged['brand_helper'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[1],0))\n","        end_whole_word = time.time()\n","        print(\"Whole word calculation finished in \", (end_whole_word - start_whole_word), \"seconds\")\n","\n","        # Add new columns with the ratio of common words in title with respect to query and ratio of common words in description with respect to query\n","        print(\"creating ratio columns...\")\n","        start_ratio = time.time()\n","        self.df_merged['ratio_title'] = self.df_merged['common_words_in_title'] / self.df_merged['len_of_query']\n","        self.df_merged['ratio_description'] = self.df_merged['common_words_in_description'] / self.df_merged['len_of_query']\n","        self.df_merged['ratio_brand'] = self.df_merged['query_in_brand'] / self.df_merged['len_of_brand']\n","        print(self.df_merged.head(3))\n","        self.df_merged.replace([np.inf, -np.inf], np.nan)\n","        self.df_merged['ratio_title'].fillna(0.0, inplace = True)\n","        self.df_merged['ratio_description'].fillna(0.0, inplace = True)\n","        print(self.df_merged.head(3))\n","\n","        self.df_merged['query_title_len_prop'] = self.df_merged['len_of_title'] / self.df_merged['len_of_query']\n","        self.df_merged['query_desc_len_prop'] = self.df_merged['len_of_description'] / self.df_merged['len_of_query']\n","        end_ratio = time.time()\n","        print(\"Ratio calculation finished in \", (end_ratio - start_ratio), \"seconds\")\n","        \n","      # Function to compute jaccard similarity\n","        def jaccard_sim(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","            set_1 = set(str_1.split())\n","            set_2 = set(str_2.split())\n","            return len(set_1.intersection(set_2)) / (float(len(set_1.union(set_2))))\n","\n","        # Add new columns with jaccard similarity of search term and product title and search term and product description\n","        print(\"creating jacard columns ....\")\n","        start_jaccard = time.time()\n","        self.df_merged['jaccard_search_and_title'] = self.df_merged['product_all_information'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[1]))\n","        self.df_merged['jaccard_search_and_description'] = self.df_merged['product_all_information'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[2]))\n","        end_jaccard = time.time()\n","        print(\"Jaccard calculation finished in \", (end_jaccard - start_jaccard), \"seconds\")\n","        return self.df_merged\n","    \n","        # create a metric as RMSE is not provided in the scoring parameter in cross-validation\n","        def fmean_squared_error(ground_truth, predictions):\n","            fmean_squared_error = mean_squared_error(ground_truth, predictions)**0.5\n","            return fmean_squared_error\n","            RMSE = make_scorer(fmean_squared_error, greater_is_better=False)\n","            return RMSE\n","\n","    def train_and_predict(self, compare_many=None):\n","        self.df_merged = self.feature_engineering()\n","        print(self.df_merged.head(2))\n","        # Drop the unecessary columns\n","        df = self.df_merged.drop(columns=['search_term','product_title','product_description','product_all_information','product_uid','brand'],axis=1)\n","        print(df.info())\n","        df_train = df.iloc[:self.len_train]\n","        print(df_train.shape)\n","        print(df_train.head(5))\n","\n","        df_test = df.iloc[self.len_train:]\n","        print(df_test.shape)\n","        id_test = df_test['id'] # keep this for exporting to csv\n","        print(id_test.shape)\n","        print(df_train.head(5))\n","\n","        X = df_train.drop(columns=['relevance', 'id']) # drop label and id\n","        y = df_train['relevance']\n","\n","        # Standard scaling\n","        scaler = StandardScaler()\n","        X = X.apply(lambda x: scaler.fit_transform(X))\n","        print(X.head(5))\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                            y,\n","                                                            train_size=0.7,\n","                                                            test_size=0.3,\n","                                                            random_state=42,\n","                                                            )\n","\n","        if compare_many:\n","            dict_regressors = {\"Linear\": LinearRegression(),\n","                              \"Ridge\": Ridge(),\n","                              \"SVR\": SVR(),\n","                              \"RF\": RandomForestRegressor(),\n","                              \"GBR\": GradientBoostingRegressor(),\n","                               \n","                              }\n","            for name, model in dict_regressors.items():\n","                model.fit(X_train, y_train)\n","                y_pred = rf.predict(X_test)\n","                mse = mean_squared_error(y_pred, y_test)\n","                rmse = np.sqrt(mse)\n","                print(name, ' RMSE is: %.4f' % rf_rmse)\n","\n","        else:\n","            # create a metric as RMSE is not provided in the scoring parameter in cross-validation\n","            def fmean_squared_error(ground_truth, predictions):\n","                fmean_squared_error = mean_squared_error(ground_truth, predictions)**0.5\n","                return fmean_squared_error\n","            RMSE = make_scorer(fmean_squared_error, greater_is_better=False)\n","\n","            rf = RandomForestRegressor(random_state=0)\n","\n","            # Set the parameter values for GridSearch\n","            param_grid = {'max_samples': [0.1, 0.2, 0.4],\n","                          'n_estimators': [100, 200, 300],\n","                          'min_samples_split': [2, 4, 6]\n","                        }\n","\n","            model = GridSearchCV(estimator= rf,\n","                                param_grid= param_grid,\n","                                n_jobs= -1,\n","                                cv= 10, \n","                                verbose= 20,\n","                                scoring= RMSE\n","                                )\n","\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_test)\n","            print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","            print(\"Random Forest's best CV score: %.4f\" % model.best_score_)\n","        \n","            # predictions = pd.DataFrame({\"id\": id_test,\n","            #                     \"relevance\": y_pred})\n","            # predictions.to_csv('predictions_rf.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqCtUyhbT2pp","colab_type":"code","colab":{}},"source":["settings = {\n","    'path_to_files': '/content/drive/My Drive/NLP', # path_to_files = \"/content/drive/My Drive/Colab Notebooks/\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDjhx5fGT2pv","colab_type":"code","colab":{}},"source":["rsp = RelevanceScorePrediction(settings)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"vIdOM6tHT2p0","colab_type":"code","outputId":"dca65f42-cb91-422e-de5d-452fd3101438","executionInfo":{"status":"error","timestamp":1589661398351,"user_tz":-180,"elapsed":62389,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# rsp.reads_and_merges()\n","\n","rsp.train_and_predict(compare_many=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The shape of train is: (74067, 5)\n","The info of train is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 74067 entries, 0 to 74066\n","Data columns (total 5 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   id             74067 non-null  int64  \n"," 1   product_uid    74067 non-null  int64  \n"," 2   product_title  74067 non-null  object \n"," 3   search_term    74067 non-null  object \n"," 4   relevance      74067 non-null  float64\n","dtypes: float64(1), int64(2), object(2)\n","memory usage: 2.8+ MB\n","None\n","The len_train is: \n","74067\n","\n"," The shape of test is: (166693, 4)\n","The info of test is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 166693 entries, 0 to 166692\n","Data columns (total 4 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             166693 non-null  int64 \n"," 1   product_uid    166693 non-null  int64 \n"," 2   product_title  166693 non-null  object\n"," 3   search_term    166693 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 5.1+ MB\n","None\n","\n"," The shape of product description is:  (124428, 2)\n","The info of product description is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 124428 entries, 0 to 124427\n","Data columns (total 2 columns):\n"," #   Column               Non-Null Count   Dtype \n","---  ------               --------------   ----- \n"," 0   product_uid          124428 non-null  int64 \n"," 1   product_description  124428 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 1.9+ MB\n","None\n","\n"," The shape of product attributes is: \n"," (2044803, 3)\n","The null values of product attributes before drop are: \n"," product_uid     155\n","name            155\n","value          2284\n","dtype: int64\n","The null values of product attributes after drop are: \n"," product_uid       0\n","name              0\n","value          2129\n","dtype: int64\n","The shape of train-test dataframe is:  (240760, 5)\n","The shape of product_brand is:  (86250, 2)\n","The shape of train_test_brands is:  (240760, 6)\n","The info of train_test_brands is: \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count   Dtype  \n","---  ------         --------------   -----  \n"," 0   id             240760 non-null  int64  \n"," 1   product_uid    240760 non-null  int64  \n"," 2   product_title  240760 non-null  object \n"," 3   search_term    240760 non-null  object \n"," 4   relevance      74067 non-null   float64\n"," 5   brand          194623 non-null  object \n","dtypes: float64(1), int64(2), object(3)\n","memory usage: 12.9+ MB\n","None\n","The info of train_test_brands after filling null values is: \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             240760 non-null  int64 \n"," 1   product_uid    240760 non-null  int64 \n"," 2   product_title  240760 non-null  object\n"," 3   search_term    240760 non-null  object\n"," 4   relevance      240760 non-null  object\n"," 5   brand          240760 non-null  object\n","dtypes: int64(2), object(4)\n","memory usage: 12.9+ MB\n","None\n","Perform text corrections with regex\n","product_title...\n","brand...\n","product_description...\n","BEHR Premium Textured DECKOVER is an innovative solid color coating. It will bring your old, weathered wood or concrete back to life. The advanced 100% acrylic resin formula creates a durable coating for your tired and worn out deck, rejuvenating to a whole new look.  For the best results, be sure to properly prepare the surface using other applicable BEHR products displayed above. California residents: see&nbsp; Proposition 65 information Revives wood and composite decks, railings, porches and boat docks, also great for concrete pool decks, patios and sidewalks100% acrylic solid color coating Resists cracking and peeling and conceals splinters and cracks up to 1/4 in. Provides a durable, mildew resistant finish Covers up to 75 sq. ft. in 2 coats per gallon Creates a textured, slip-resistant finish For best results, prepare with the appropriate BEHR product for your wood or concrete surface Actual paint colors may vary from on-screen and printer representations Colors available to be tinted in most stores Online Price includes Paint Care fee in the following states: CA, CO, CT, ME, MN, OR, RI, VT\n","Simple regex finished in  11.240453720092773 seconds\n","Removing stopwords...\n","The stopwords are:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","search_term...\n","product_title...\n","brand...\n","product_description...\n","behr premium textured deckover innovative solid color coating . bring old , weathered wood concrete back life . advanced 100 % acrylic resin formula creates durable coating tired worn deck , rejuvenating whole new look . best results , sure properly prepare surface using applicable behr products displayed . california residents : see & nbsp ; proposition 65 information revives wood composite decks , railings , porches boat docks , also great concrete pool decks , patios sidewalks100 % acrylic solid color coating resists cracking peeling conceals splinters cracks 1/4 . provides durable , mildew resistant finish covers 75 sq . ft. 2 coats per gallon creates textured , slip-resistant finish best results , prepare appropriate behr product wood concrete surface actual paint colors may vary on-screen printer representations colors available tinted stores online price includes paint care fee following states : ca , co , ct , , mn , , ri , vt\n","Stopwords removal finished in  273.3205931186676 seconds\n","----------------------------------------------------\n","Removing punctuations...\n","The punctuations are:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","search_term...\n","product_title...\n","brand...\n","product_description...\n","behr premium textured deckover innovative solid color coating  bring old  weathered wood concrete back life  advanced 100  acrylic resin formula creates durable coating tired worn deck  rejuvenating whole new look  best results  sure properly prepare surface using applicable behr products displayed  california residents  see  nbsp  proposition 65 information revives wood composite decks  railings  porches boat docks  also great concrete pool decks  patios sidewalks100  acrylic solid color coating resists cracking peeling conceals splinters cracks 14  provides durable  mildew resistant finish covers 75 sq  ft 2 coats per gallon creates textured  slipresistant finish best results  prepare appropriate behr product wood concrete surface actual paint colors may vary onscreen printer representations colors available tinted stores online price includes paint care fee following states  ca  co  ct   mn   ri  vt\n","Punctuation removal finished in  1.8774456977844238 seconds\n","----------------------------------------------------\n","Creating vocabulary for spell correction...\n","column  1\n","column  2\n","column  3\n","WORDS:  [('x', 157913), ('ft', 93097), ('use', 78091)]\n","Spell correcting...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hQFuBw2J62jI","colab":{}},"source":["# def cosine_sim(vec_1, vec_2):\n","#     return vec_1.dot(vec_2)/np.sqrt(vec_1.dot(vec_1) * vec_2.dot(vec_2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qBTLVpyaDpWp"},"source":["CountVectorizer for train"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CfYHM71Wk54f","colab":{}},"source":["# THIS HAS NOT BEEN TESTED YET. WE CAN DO THIS TO FIND TFxIDF\n","# cv = CountVectorizer()\n","# bag_of_words_transformer = cv.fit(df_train['product_description'])\n","# print(len(bag_of_words_transformer.vocabulary_)) # total number of vocabulary\n","\n","# # convert the product_description column\n","# description_bow = bag_of_words_transformer.transform(df_train['product_description'])\n","# print('The shape of sparse matrix is:',description_bow.shape)\n","# print('The number of non zero occurances is:',description_bow.nnz)\n","# sparsity = (100.0 * description_bow.nnz / (description_bow.shape[0] * description_bow.shape[1]))\n","# print('The sparsity is: {} %'.format(round(100 * sparsity, ndigits=4))) #how many zeros there are in the matrix\n","\n","# new_train_df = pd.DataFrame(description_bow.toarray(), columns=cv.get_feature_names())\n","# new_train_df.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZcI-sVyxtF8u","colab":{}},"source":["rf = RandomForestRegressor(random_state=0)\n","\n","# Set the parameter values for GridSearch\n","param_grid = {'max_samples': [0.1, 0.2, 0.4],\n","              'n_estimators': [100, 200, 300],\n","              'min_samples_split': [2, 4, 6]\n","             }\n","\n","model = GridSearchCV(estimator= rf,\n","                    param_grid= param_grid,\n","                    n_jobs= -1,\n","                    cv= 10, \n","                    verbose= 20,\n","                    scoring= RMSE\n","                    )\n","\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","print(\"Random Forest's best CV score: %.4f\" % model.best_score_)\n","\n","# # Gradient Boosting regressor. It has not been used in conjunction to Bagging regressor\n","# gb = GradientBoostingRegressor(random_state=0)\n","# gb.fit(X_train, y_train)\n","# y_pred = gb.predict(X_test)\n","\n","# model = GridSearchCV(estimator= clf,\n","#                                 param_grid= param_grid,\n","#                                 n_jobs= -1,\n","#                                 cv= 2, \n","#                                 verbose= 20,\n","#                                 scoring= RMSE)\n","# model.fit(X_train, y_train)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sNVwwJXbtbZG"},"source":["3. Bagging Regressor to improve Random Forest"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aud7y-wttePq","colab":{}},"source":["# clf = BaggingRegressor(base_estimator= rf, random_state=1)\n","# clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# # Set the parameter values for GridSearch\n","# param_grid = {'max_samples': [0.1, 0.2],\n","#               'n_estimators': [10, 100, 150],\n","#               'min_samples_split': [2, 4, 6]\n","#              }\n","\n","# model = GridSearchCV(\n","#     estimator= rf,\n","#     param_grid= param_grid,\n","#     n_jobs= -1,\n","#     cv= 10, \n","#     verbose= 20,\n","#     scoring= RMSE\n","# )\n","\n","# model.fit(X_train, y_train)\n","# y_pred = model.predict(X_test)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5bJkdA9LzsR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}