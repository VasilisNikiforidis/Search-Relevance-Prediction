{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Coursework_2020_extra.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"88KbEPMAmbxF","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"87f9f7cb-5c9f-4096-a21a-031ab789f8a1","executionInfo":{"status":"ok","timestamp":1590254643344,"user_tz":-180,"elapsed":1760,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TDd4BKiQmvSg","outputId":"6ad92bc3-ee6d-475c-9973-28d3b0c2ac88","executionInfo":{"status":"ok","timestamp":1590254643716,"user_tz":-180,"elapsed":2118,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import os\n","import re\n","import string\n","import json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","import requests\n","from random import randint\n","from matplotlib import pyplot\n","%matplotlib inline\n","import warnings\n","from collections import Counter\n","warnings.filterwarnings(\"ignore\")\n","try:\n","    from inflection import singularize\n","except:\n","    !pip install inflection\n","    from inflection import singularize\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.util import ngrams\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import BaggingRegressor\n","from xgboost import XGBRegressor\n","from xgboost import plot_importance\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import make_scorer"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tTN-4EN_sFxU","colab_type":"code","colab":{}},"source":["# Global items\n","try:\n","    with open('/content/drive/My Drive/NLP/spell_check_dict.json') as f:\n","        spell_check_dict = json.load(f)\n","    spelling_dictionary_imported = True\n","except:\n","    spell_check_dict = dict()\n","    spelling_dictionary_imported = False\n","    \n","randomSeed = 5\n","\n","# create a metric as RMSE is not provided in the scoring parameter in cross-validation\n","def fmean_squared_error(ground_truth, predictions):\n","    fmean_squared_error = np.sqrt(mean_squared_error(ground_truth, predictions))\n","    return fmean_squared_error\n","RMSE = make_scorer(fmean_squared_error, greater_is_better=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZHnD-KvWnKka","scrolled":true,"colab":{}},"source":["class RelevanceScorePrediction:\n","    def __init__(self, settings):\n","        self.path_to_files = settings['path_to_files']        \n","        self.train_path = self.path_to_files + \"/train.csv\"\n","        self.test_path = self.path_to_files + \"/test.csv\"\n","        self.product_descriptions_path = self.path_to_files + \"/product_descriptions.csv\"\n","        self.attributes_path = self.path_to_files + \"/attributes.csv\"\n","        \n","        self.df_train = pd.read_csv(self.train_path, encoding='ISO-8859-1', header=0)\n","        self.df_test = pd.read_csv(self.test_path, encoding='ISO-8859-1', header=0)\n","        self.df_product_desc = pd.read_csv(self.product_descriptions_path, header=0)\n","        self.df_product_attr = pd.read_csv(self.attributes_path, header=0)\n","\n","    # Functions that reads the dataframes  \n","    def reads_and_merges(self):\n","        print('The shape of train is:', self.df_train.shape)\n","        print('The info of train is: ')\n","        print(self.df_train.info())\n","        self.len_train = self.df_train.shape[0]\n","        print('The len_train is: ')\n","        print(self.len_train)\n","        \n","        print('\\n The shape of test is:', self.df_test.shape)\n","        print('The info of test is: ')\n","        print(self.df_test.info())\n","        \n","        print('\\n The shape of product description is: ', self.df_product_desc.shape)\n","        print('The info of product description is: ')\n","        print(self.df_product_desc.info())\n","\n","        print('\\n The shape of product attributes is: \\n', self.df_product_attr.shape)\n","        print('The null values of product attributes before drop are: \\n', self.df_product_attr.isnull().sum())\n","\n","        self.df_product_attr = self.df_product_attr.dropna(how=\"all\")\n","        self.df_product_attr[\"product_uid\"] = self.df_product_attr[\"product_uid\"].astype(\"int64\")\n","        print('The null values of product attributes after drop are: \\n', self.df_product_attr.isnull().sum())\n","        \n","        # merge train with test\n","        self.df_train_test = pd.concat([self.df_train, self.df_test], axis=0, ignore_index=True)\n","        print('The shape of train-test dataframe is: ', self.df_train_test.shape)\n","        \n","        # For attributes table, we are only interested in brand names which could be included in search queries.\n","        self.df_product_brand = self.df_product_attr[self.df_product_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n","        print('The shape of product_brand is: ', self.df_product_brand.shape)\n","        \n","        # merge train_test with product attributes\n","        self.df_train_test_brands = pd.merge(self.df_train_test, self.df_product_brand, how='left', on='product_uid')\n","        print('The shape of train_test_brands is: ', self.df_train_test_brands.shape)\n","        print('The info of train_test_brands is: ')\n","        print(self.df_train_test_brands.info())\n","        self.df_train_test_brands[\"brand\"] = self.df_train_test_brands[\"brand\"].astype(str)\n","        self.df_train_test_brands.fillna(\"unknown\", inplace=True)\n","        print('The info of train_test_brands after filling null values is: ')\n","        print(self.df_train_test_brands.info())\n","        \n","        return self.df_train, self.df_test, self.df_product_desc, self.df_product_attr, self.df_train_test, self.df_train_test_brands\n","\n","    def preprocessing(self):\n","        self.df_train, self.df_test, self.df_product_desc, self.df_product_attr, self.df_train_test, self.df_train_test_brands = self.reads_and_merges()\n","        \n","        # Function to perform simole regex\n","        def simple_regex(text, regex1, sub1, regex2, sub2, regex3, sub3):\n","            text = re.sub(regex1, sub1, text) # add space after . if it doesn't exist.also avoid add space after the last .\n","            text = re.sub(regex2, sub2, text) # do this also for ;\n","            text = re.sub(regex3, sub3, text) # add space Capital words that are in the form textTex\n","            return text\n","\n","        # Perfom simple regex\n","        print('Perform text corrections with regex')\n","        start_regex = time.time()\n","        regex1 = r\"\\.(?!\\s)(?!$)\"\n","        sub1 = \". \"\n","        regex2 = r\"\\;(?!\\s)(?!$)\"\n","        sub2 = \"; \"\n","        regex3 = r\"([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))\"\n","        sub3 = r\"\\1 \"\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print(\"product_description...\")\n","        print('initial: \\n', self.df_product_desc['product_description'][1])\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: simple_regex(x, regex1, sub1, regex2, sub2, regex3, sub3))\n","        print('after regex: \\n', self.df_product_desc['product_description'][1])\n","        end_regex = time.time()\n","        print(\"Simple regex finished in \", (end_regex - start_regex), \"seconds\")\n","\n","        # Function to correct measurement units\n","        def measurement_unit_correction(text):\n","            \"\"\"\n","            This function preprocess a string. Corrects measurement units.\n","            Input: A string\n","            Output: A string with corrected measurements\n","            Example: text = measurement_unit_correction(text)\n","            \"\"\"\n","            if isinstance(text, str):\n","\n","                metric = text.lower()\n","\n","                metric = metric.replace(\"centimeters\",\" cm.\")    \n","\n","                metric = metric.replace(\"millimeters\",\" mm.\")\n","\n","                metric = metric.replace(\"'\",\" in. \") \n","                metric = metric.replace(\"inches\",\" in. \") \n","                metric = metric.replace(\"inch\",\" in. \")\n","                metric = metric.replace(\"''\",\" ft. \") \n","                metric = metric.replace(\"feet\",\" ft. \") \n","                metric = metric.replace(\"foot\",\" ft. \") \n","                metric = metric.replace(\"sq ft\",\" sq.ft. \") \n","                metric = metric.replace(\"sqft \",\" sq.ft. \")\n","                metric = metric.replace(\"sq. ft\",\" sq.ft. \") \n","                metric = metric.replace(\"sq ft.\",\" sq.ft. \") \n","                metric = metric.replace(\"sq feet\",\" sq.ft. \") \n","                metric = metric.replace(\"square feet\",\" sq.ft. \") \n","\n","                metric = metric.replace(\"pounds\",\" lb. \")\n","                metric = metric.replace(\"pound\",\" lb. \") \n","                metric = metric.replace(\"lbs \",\" lb. \") \n","                metric = metric.replace(\"lbs.\",\" lb. \") \n","\n","                metric = metric.replace(\" x \",\" xby \")\n","                metric = metric.replace(\"*\",\" xby \")\n","                metric = metric.replace(\"by\",\" xby \")\n","                metric = metric.replace(\"x0\",\" xby 0 \")\n","                metric = metric.replace(\"x1\",\" xby 1 \")\n","                metric = metric.replace(\"x2\",\" xby 2 \")\n","                metric = metric.replace(\"x3\",\" xby 3 \")\n","                metric = metric.replace(\"x4\",\" xby 4 \")\n","                metric = metric.replace(\"x5\",\" xby 5 \")\n","                metric = metric.replace(\"x6\",\" xby 6 \")\n","                metric = metric.replace(\"x7\",\" xby 7 \")\n","                metric = metric.replace(\"x8\",\" xby 8 \")\n","                metric = metric.replace(\"x9\",\" xby 9 \")\n","                metric = metric.replace(\"0x\",\" 0 xby \")\n","                metric = metric.replace(\"1x\",\" 1 xby \")\n","                metric = metric.replace(\"2x\",\" 2 xby \")\n","                metric = metric.replace(\"3x\",\" 3 xby \")\n","                metric = metric.replace(\"4x\",\" 4 xby \")\n","                metric = metric.replace(\"5x\",\" 5 xby \")\n","                metric = metric.replace(\"6x\",\" 6 xby \")\n","                metric = metric.replace(\"7x\",\" 7 xby \")\n","                metric = metric.replace(\"8x\",\" 8 xby \")\n","                metric = metric.replace(\"9x\",\" 9 xby \")\n","            \n","                metric = metric.replace(\"gallon\",\" gal. \")\n","                metric = metric.replace(\"gallons\",\" gal. \") \n","\n","                metric = metric.replace(\"ounces\",\" oz. \")\n","                metric = metric.replace(\"ounce\",\" oz. \")\n","                \n","                metric = metric.replace(\"°\",\" deg. \")\n","                metric = metric.replace(\"degrees\",\" deg. \")\n","                metric = metric.replace(\"degree\",\" deg. \")\n","                \n","                metric = metric.replace(\"volts\",\" volt. \")\n","\n","                metric = metric.replace(\"watts\",\" watt. \")\n","                metric = metric.replace(\"watt\",\" watt. \")\n","\n","                metric = metric.replace(\"ampere\",\" amp. \")\n","                metric = metric.replace(\"amps\",\" amp. \")\n","\n","                return metric\n","\n","        print('Measurement unit correction...')\n","        start_units = time.time()\n","        print('search term...')\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: measurement_unit_correction(x))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: measurement_unit_correction(x))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: measurement_unit_correction(x))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: measurement_unit_correction(x))\n","        end_units = time.time()\n","        print(\"Measurement unit correction finished in \", (end_units - start_units), \"seconds\")\n","\n","        # Function to remove stopwords from a string\n","        def stopwords_removal(text, cachedStopWords):\n","            \"\"\"\n","            This function preprocess a string. Removes stopwords.\n","            Input: A string\n","            Output: A string cleaned from stop_words\n","            Example: text = stopwords_removal(text)\n","            \"\"\"\n","            if not isinstance(text, str):\n","                return text\n","\n","            words = nltk.word_tokenize(text.lower())\n","            no_stopwords = [word for word in words if word not in cachedStopWords]\n","            return ' '.join(no_stopwords)\n","    \n","        # stopwords removal\n","        print(\"Removing stopwords...\")\n","        start_stopwords = time.time()\n","        cachedStopWords = stopwords.words('english')\n","        print('The stopwords are: ', cachedStopWords) # to see the english stopwords\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: stopwords_removal(x,cachedStopWords))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: stopwords_removal(x, cachedStopWords))\n","        print(self.df_product_desc['product_description'][1])\n","        end_stopwords = time.time()\n","        print(\"Stopwords removal finished in \", (end_stopwords - start_stopwords), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        # Function to remove punctuations from a string\n","        def punctuations_removal(text, translate_table_punctuations):\n","            \"\"\"\n","            This function preprocess a string. Removes punctuations.\n","            Input: A string\n","            Output: A string cleaned from punctuations\n","            Example: text = punctuation_removal(text)\n","            look also this: https://stackoverflow.com/questions/43935592/add-space-after-full-stops\n","            \"\"\"\n","            if not isinstance(text, str):\n","                return text\n","            return text.translate(translate_table_punctuations)\n","\n","        # punctuation removal\n","        print(\"Removing punctuations...\")\n","        start_punc = time.time()\n","        punctuations = string.punctuation\n","        print('The punctuations are: ', punctuations) # to see the punctuations\n","        translate_table_punctuations = str.maketrans(\"\",\"\",punctuations)\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: punctuations_removal(x, translate_table_punctuations))\n","        print(self.df_product_desc['product_description'][1])\n","        end_punc = time.time()\n","        print(\"Punctuation removal finished in \", (end_punc - start_punc), \"seconds\")\n","        print(\"----------------------------------------------------\")\n","\n","        \n","        # Function to correct spelling errors\n","        def spell_check(text):\n","            if spelling_dictionary_imported:\n","                return spell_check_dict[text]\n","            \n","            def P(word, N=sum(WORDS.values())): \n","                \"Probability of `word`.\"\n","                return WORDS[word] / N\n","\n","            def correction(word): \n","                \"Most probable spelling correction for word.\"\n","                return max(candidates(word), key=P)\n","\n","            def candidates(word): \n","                \"Generate possible spelling corrections for word.\"\n","                return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","            def known(words): \n","                \"The subset of `words` that appear in the dictionary of WORDS.\"\n","                return set(w for w in words if w in WORDS)\n","\n","            def edits1(word):\n","                \"All edits that are one edit away from `word`.\"\n","                letters    = 'abcdefghijklmnopqrstuvwxyz'\n","                splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","                deletes    = [L + R[1:]               for L, R in splits if R]\n","                transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","                replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","                inserts    = [L + c + R               for L, R in splits for c in letters]\n","                return set(deletes + transposes + replaces + inserts)\n","\n","            def edits2(word): \n","                \"All edits that are two edits away from `word`.\"\n","                return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","\n","            lower_case_input = text.lower()\n","            tokens = word_tokenize(lower_case_input)\n","            corrected = [correction(token) for token in tokens]\n","            spell_check_dict[lower_case_input] = ' '.join(corrected)\n","            return ' '.join(corrected)\n","\n","        # Spell Correction\n","        if not spelling_dictionary_imported:\n","            print(\"Creating vocabulary for spell correction...\")\n","            wordsArr = []\n","            vocabulary_columns = [self.df_train_test_brands[\"product_title\"], self.df_product_desc[\"product_description\"], self.df_train_test_brands[\"brand\"]]\n","            for column in vocabulary_columns:\n","                for entry in column:\n","                    words = word_tokenize(entry.lower())\n","                    for word in words:\n","                        wordsArr.append(word)\n","\n","            WORDS = Counter(wordsArr)\n","            print(\"Top 5 most common words found in description, title and brand: \", WORDS.most_common(5))\n","        \n","        print(\"Spell correcting...\")\n","        start_spell = time.time()\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: spell_check(x))\n","        end_spell = time.time()\n","        print(\"Spell correction finished in \", (end_spell - start_spell), \"seconds\")\n","\n","        # Store the dictionary to file to use as static from now on\n","        if not spelling_dictionary_imported:\n","            with open('/content/drive/My Drive/spell_check_dict.json', 'w') as j:\n","                j.write(json.dumps(spell_check_dict))\n","        print(\"----------------------------------------------------\")\n","\n","\n","        def singulairze_text(text):\n","            return ' '.join([singularize(word) for word in text.split()])\n","\n","        print(\"Singularizing...\")\n","        start_singularize = time.time()\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: singulairze_text(x))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: singulairze_text(x))\n","        print(self.df_product_desc['product_description'][1])\n","        end_singularize = time.time()\n","        print(\"Singularizing finished in \", (end_singularize - start_singularize), \"seconds\")\n","\n","        # Function to perform stemming\n","        def simple_stemming(text, snowball_stemmer):\n","            if not isinstance(text, str):\n","                return text\n","            tokens = word_tokenize(text.lower())\n","            stemmed = [snowball_stemmer.stem(word) for word in tokens]\n","            return ' '.join(stemmed)\n","\n","        # Perform simple stemming\n","        print(\"Performing simple stemming...\")\n","        start_stem = time.time()\n","        snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n","        print(\"search_term...\")\n","        self.df_train_test_brands['search_term'] = self.df_train_test_brands['search_term'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"product_title...\")\n","        self.df_train_test_brands['product_title'] = self.df_train_test_brands['product_title'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"brand...\")\n","        self.df_train_test_brands['brand'] = self.df_train_test_brands['brand'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(\"product_description...\")\n","        self.df_product_desc['product_description'] = self.df_product_desc['product_description'].map(lambda x: simple_stemming(x, snowball_stemmer))\n","        print(self.df_product_desc['product_description'][1])\n","        end_stem = time.time()\n","        print(\"Simple stemming finished in \", (end_stem - start_stem), \"seconds\")\n","        \n","        self.df_merged = self.df_train_test_brands.merge(self.df_product_desc, how='left', on='product_uid')\n","        print('\\n The shape of train_test with products_description is: ', self.df_merged.shape)\n","        print(self.df_merged.info())\n","        return self.df_merged\n","    \n","    # Function to constract new features\n","    def feature_engineering(self):\n","        self.df_merged = self.preprocessing()\n","        \n","        # Add new columns with the length of the column\n","        print(\"creating len columns...\")\n","        start_length = time.time()\n","        self.df_merged['len_of_query'] = self.df_merged['search_term'].map(lambda x: len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_title'] = self.df_merged['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_description'] = self.df_merged['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n","        self.df_merged['len_of_brand'] = self.df_merged['brand'].apply(lambda x:len(x.split())).astype(np.int64)\n","        end_length = time.time()\n","        print(\"Length calculation finished in \", (end_length - start_length), \"seconds\")\n","\n","        # Add a product_all_info column summarizing the attributes of products seperating by TAB\n","        print(\"creating combination of columns...\")\n","        self.df_merged['product_all_information'] = self.df_merged['search_term'] + \"\\t\" + self.df_merged['product_title'] + \"\\t\" + self.df_merged['product_description']\n","        self.df_merged['brand_helper'] = self.df_merged['search_term'] + \"\\t\" + self.df_merged['brand'] + \"\\t\" + self.df_merged['product_title']\n","\n","        # Function to find number of common words.\n","        def num_common_words(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","            return sum(int(str_2.find(word)>=0) for word in str_1.split())\n","\n","        # Add new columns with common words of the search term with product title and the search term with description\n","        print(\"creating common_words title/description...\")\n","        start_common_words = time.time()\n","        self.df_merged['common_words_in_title'] = self.df_merged['product_all_information'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[1]))\n","        self.df_merged['common_words_in_description'] = self.df_merged['product_all_information'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[2]))\n","        self.df_merged['common_words_in_brand'] = self.df_merged['brand_helper'].map(lambda x: num_common_words(x.split('\\t')[0], x.split('\\t')[1]))\n","        end_common_words = time.time()\n","        print(\"Common words calculation finished in \", (end_common_words - start_common_words), \"seconds\")\n","\n","        # Function to find common phrase\n","        def num_whole_word(str_1, str_2, i):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","\n","            str_1, str_2 = str_1.strip(), str_2.strip()\n","            count = 0\n","            while i < len(str_2):\n","                i = str_2.find(str_1, i)\n","                if i == -1:\n","                    return count\n","                else:\n","                    count += 1\n","                    i += len(str_1)\n","            return count\n","\n","        # Add new columns with number of times the entire search term appears in product title and number of times the entire search term appears in product description.\n","        # print(\"creating query_in_X columns...\")\n","        # start_whole_word = time.time()\n","        # print(\"query_in_title...\")\n","        # self.df_merged['query_in_title'] = self.df_merged['product_all_information'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[1],0))\n","        # print(\"query_in_description...\")\n","        # self.df_merged['query_in_description'] = self.df_merged['product_all_information'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[2],0))\n","        # print(\"query_in_brand...\")\n","        # self.df_merged['query_in_brand'] = self.df_merged['brand_helper'].map(lambda x: num_whole_word(x.split('\\t')[0], x.split('\\t')[1],0))\n","        # end_whole_word = time.time()\n","        # print(\"Whole word calculation finished in \", (end_whole_word - start_whole_word), \"seconds\")\n","\n","        # Add new columns with the ratio of common words in title with respect to query and ratio of common words in description with respect to query\n","        print(\"creating ratio columns...\")\n","        start_ratio = time.time()\n","        self.df_merged['ratio_title'] = self.df_merged['common_words_in_title'] / self.df_merged['len_of_query']\n","        self.df_merged['ratio_description'] = self.df_merged['common_words_in_description'] / self.df_merged['len_of_query']\n","        # self.df_merged['ratio_brand'] = self.df_merged['query_in_brand'] / self.df_merged['len_of_brand']\n","\n","        self.df_merged['query_title_len_prop'] = self.df_merged['len_of_title'] / self.df_merged['len_of_query']\n","        self.df_merged['query_desc_len_prop'] = self.df_merged['len_of_description'] / self.df_merged['len_of_query']\n","        self.df_merged = self.df_merged.replace([np.inf, -np.inf], np.nan)\n","        print(self.df_merged.info())\n","        self.df_merged['ratio_title'].fillna(0.0, inplace = True)\n","        self.df_merged['ratio_description'].fillna(0.0, inplace = True)\n","        self.df_merged['query_title_len_prop'].fillna(0.0, inplace = True)\n","        self.df_merged['query_desc_len_prop'].fillna(0.0, inplace = True)\n","        print(self.df_merged.info())\n","        end_ratio = time.time()\n","        print(\"Ratio calculation finished in \", (end_ratio - start_ratio), \"seconds\")\n","        \n","      # Function to compute jaccard similarity\n","        def jaccard_sim(str_1, str_2):\n","            if not isinstance(str_1, str) or not isinstance(str_2, str):\n","                return 0\n","            set_1 = set(str_1.split())\n","            set_2 = set(str_2.split())\n","            if len(set_1.union(set_2)) == 0:\n","              return 0\n","            return len(set_1.intersection(set_2)) / (float(len(set_1.union(set_2))))\n","\n","        # Add new columns with jaccard similarity of search term and product title and search term and product description\n","        print(\"creating jacard columns ....\")\n","        start_jaccard = time.time()\n","        self.df_merged['jaccard_search_and_title'] = self.df_merged['product_all_information'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[1])).round(3)\n","        self.df_merged['jaccard_search_and_description'] = self.df_merged['product_all_information'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[2])).round(3)\n","        self.df_merged['jaccard_search_and_brand'] = self.df_merged['brand_helper'].map(lambda x: jaccard_sim(x.split('\\t')[0], x.split('\\t')[1])).round(3)\n","        end_jaccard = time.time()\n","        print(\"Jaccard calculation finished in \", (end_jaccard - start_jaccard), \"seconds\")\n","\n","        # Function to compute the number of common ngrams\n","        def common_ngrams(str_1, str_2, n):\n","            bigrams_1 = ngrams(str_1.lower().split(), n)\n","            bigrams_2 = ngrams(str_2.lower().split(), n)\n","            common = []\n","            for grams_1 in bigrams_1:\n","                if grams_1 in bigrams_2:\n","                    common.append(grams_1)\n","            if not common:\n","              return 0\n","            else:\n","              return len(common)\n","\n","        # Add new columns with number of common ngrams\n","        print(\"creating ngrams columns ....\")\n","        start_ngrams = time.time()\n","        self.df_merged['ngrams_search_and_title'] = self.df_merged['product_all_information'].map(lambda x: common_ngrams(x.split('\\t')[0], x.split('\\t')[1], 2))\n","        self.df_merged['ngrams_search_and_description'] = self.df_merged['product_all_information'].map(lambda x: common_ngrams(x.split('\\t')[0], x.split('\\t')[2], 2))\n","        self.df_merged['ngrams_search_and_brand'] = self.df_merged['brand_helper'].map(lambda x: common_ngrams(x.split('\\t')[0], x.split('\\t')[2], 2))\n","        end_ngrams = time.time()\n","        print(\"ngrams calculation finished in \", (end_ngrams - start_ngrams), \"seconds\")\n","\n","        return self.df_merged\n","    \n","\n","    def train_and_predict(self, compare_many=False, grid=True):\n","        self.df_merged = self.feature_engineering()\n","        print(self.df_merged.head(2))\n","        # Drop the unecessary columns\n","        df = self.df_merged.drop(columns=['search_term','product_title','product_description','product_all_information','product_uid','brand','brand_helper'],axis=1)\n","        print(df.info())\n","\n","        df_train = df.iloc[:self.len_train]\n","        # df_train.reset_index(drop=True, inplace=True)\n","        print(df_train.shape)\n","        print(df_train.head(5))\n","\n","        df_test = df.iloc[self.len_train:]\n","        # df_test.reset_index(drop=True, inplace=True)\n","        print(df_test.shape)\n","        id_test = df_test['id'] # keep this for exporting to csv\n","        print(id_test.shape)\n","        print(df_test.head(5))\n","\n","        X = df_train.drop(columns=['relevance', 'id']) # drop label and id\n","        y = df_train['relevance']\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                            y,\n","                                                            train_size=0.7,\n","                                                            test_size=0.3,\n","                                                            random_state=randomSeed,\n","                                                            )\n","        print('X_train shape ', X_train.shape)\n","        print('X test shape ', X_test.shape)\n","\n","        if compare_many:\n","            dict_regressors = {\"Linear\": LinearRegression(),\n","                              \"Ridge\": Ridge(),\n","                              \"SVR\": SVR(),\n","                              \"RF\": RandomForestRegressor(),\n","                              \"GBR\": GradientBoostingRegressor(),\n","                              \"XGBoost\": XGBRegressor(),\n","                              }\n","\n","            for name, model in dict_regressors.items():\n","                model.fit(X_train, y_train)\n","                y_pred = model.predict(X_test)\n","                mse = mean_squared_error(y_pred, y_test)\n","                rmse = np.sqrt(mse)\n","                print(name, ' RMSE is: %.4f' % rmse)\n","\n","        elif grid:\n","            # gb = GradientBoostingRegressor(random_state=randomSeed)\n","\n","            # # Set the parameter values for GridSearch\n","            # param_grid = {'n_estimators': [50, 100],\n","            #               'max_depth': [1, 2]\n","            #              }\n","            # model = GridSearchCV(estimator= gb,\n","            #                     param_grid= param_grid,\n","            #                     n_jobs= -1,\n","            #                     cv= 10, \n","            #                     verbose= 20,\n","            #                     scoring= RMSE)\n","\n","            # model.fit(X_train, y_train)\n","            # y_pred = model.predict(X_test)\n","            # print(\"GB: Best parameters found by grid search:\", model.best_params_)\n","            # print(\"Gradient Boosting's best CV score: %.4f\" % model.best_score_)\n","\n","            # print('-----------------------------------------------------------')\n","\n","            xgboost = XGBRegressor(random_state=randomSeed)\n","            param_grid = {'min_child_weight': [1, 3, 5, 7],\n","                          'max_depth': [5, 6, 7],\n","                          'subsample ': [0.5, 0.6, 0.7],\n","                          'colsample_bytree ': [6, 7, 8]\n","                         }\n","            model = GridSearchCV(estimator= xgboost,\n","                                param_grid= param_grid,\n","                                n_jobs= -1,\n","                                cv= 5, \n","                                verbose= 20,\n","                                scoring= RMSE)\n","\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_test)\n","\n","            plot_importance(model)\n","            pyplot.show()\n","\n","            feat_importances = pd.Series(model.feature_importances_, index=df.columns)\n","            feat_importances = feat_importances.nlargest(19)\n","            feat_importances.plot(kind='barh' , figsize=(10,10)) \n","\n","            # XGB: Best parameters found by grid search: {'colsample_bytree ': 0.75, 'max_depth': 6, 'min_child_weight': 6, 'subsample ': 0.75}\n","            # XGB's best CV score: -0.4799\n","            \n","            print(\"XGB: Best parameters found by grid search:\", model.best_params_)\n","            print(\"XGB's best CV score: %.4f\" % model.best_score_)\n","            # predictions = pd.DataFrame({\"id\": id_test,\n","            #                     \"relevance\": y_pred})\n","            # predictions.to_csv('predictions_rf.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqCtUyhbT2pp","colab_type":"code","colab":{}},"source":["settings = {\n","    'path_to_files': '/content/drive/My Drive/NLP', # path_to_files = \"/content/drive/My Drive/NLP\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDjhx5fGT2pv","colab_type":"code","colab":{}},"source":["rsp = RelevanceScorePrediction(settings)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"vIdOM6tHT2p0","colab_type":"code","outputId":"91ea8561-2be9-44c9-a31b-e47f360619a5","executionInfo":{"status":"ok","timestamp":1590099834301,"user_tz":-180,"elapsed":1945323,"user":{"displayName":"Vasileios Nikiforidis","photoUrl":"","userId":"06386003786955673687"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# rsp.train_and_predict(compare_many=True)\n","rsp.train_and_predict(grid=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The shape of train is: (74067, 5)\n","The info of train is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 74067 entries, 0 to 74066\n","Data columns (total 5 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   id             74067 non-null  int64  \n"," 1   product_uid    74067 non-null  int64  \n"," 2   product_title  74067 non-null  object \n"," 3   search_term    74067 non-null  object \n"," 4   relevance      74067 non-null  float64\n","dtypes: float64(1), int64(2), object(2)\n","memory usage: 2.8+ MB\n","None\n","The len_train is: \n","74067\n","\n"," The shape of test is: (166693, 4)\n","The info of test is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 166693 entries, 0 to 166692\n","Data columns (total 4 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             166693 non-null  int64 \n"," 1   product_uid    166693 non-null  int64 \n"," 2   product_title  166693 non-null  object\n"," 3   search_term    166693 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 5.1+ MB\n","None\n","\n"," The shape of product description is:  (124428, 2)\n","The info of product description is: \n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 124428 entries, 0 to 124427\n","Data columns (total 2 columns):\n"," #   Column               Non-Null Count   Dtype \n","---  ------               --------------   ----- \n"," 0   product_uid          124428 non-null  int64 \n"," 1   product_description  124428 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 1.9+ MB\n","None\n","\n"," The shape of product attributes is: \n"," (2044803, 3)\n","The null values of product attributes before drop are: \n"," product_uid     155\n","name            155\n","value          2284\n","dtype: int64\n","The null values of product attributes after drop are: \n"," product_uid       0\n","name              0\n","value          2129\n","dtype: int64\n","The shape of train-test dataframe is:  (240760, 5)\n","The shape of product_brand is:  (86250, 2)\n","The shape of train_test_brands is:  (240760, 6)\n","The info of train_test_brands is: \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count   Dtype  \n","---  ------         --------------   -----  \n"," 0   id             240760 non-null  int64  \n"," 1   product_uid    240760 non-null  int64  \n"," 2   product_title  240760 non-null  object \n"," 3   search_term    240760 non-null  object \n"," 4   relevance      74067 non-null   float64\n"," 5   brand          194623 non-null  object \n","dtypes: float64(1), int64(2), object(3)\n","memory usage: 12.9+ MB\n","None\n","The info of train_test_brands after filling null values is: \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             240760 non-null  int64 \n"," 1   product_uid    240760 non-null  int64 \n"," 2   product_title  240760 non-null  object\n"," 3   search_term    240760 non-null  object\n"," 4   relevance      240760 non-null  object\n"," 5   brand          240760 non-null  object\n","dtypes: int64(2), object(4)\n","memory usage: 12.9+ MB\n","None\n","Perform text corrections with regex\n","product_title...\n","brand...\n","product_description...\n","initial: \n"," BEHR Premium Textured DECKOVER is an innovative solid color coating. It will bring your old, weathered wood or concrete back to life. The advanced 100% acrylic resin formula creates a durable coating for your tired and worn out deck, rejuvenating to a whole new look.  For the best results, be sure to properly prepare the surface using other applicable BEHR products displayed above.California residents: see&nbsp;Proposition 65 informationRevives wood and composite decks, railings, porches and boat docks, also great for concrete pool decks, patios and sidewalks100% acrylic solid color coatingResists cracking and peeling and conceals splinters and cracks up to 1/4 in.Provides a durable, mildew resistant finishCovers up to 75 sq. ft. in 2 coats per gallonCreates a textured, slip-resistant finishFor best results, prepare with the appropriate BEHR product for your wood or concrete surfaceActual paint colors may vary from on-screen and printer representationsColors available to be tinted in most storesOnline Price includes Paint Care fee in the following states: CA, CO, CT, ME, MN, OR, RI, VT\n","after regex: \n"," BEHR Premium Textured DECKOVER is an innovative solid color coating. It will bring your old, weathered wood or concrete back to life. The advanced 100% acrylic resin formula creates a durable coating for your tired and worn out deck, rejuvenating to a whole new look.  For the best results, be sure to properly prepare the surface using other applicable BEHR products displayed above. California residents: see&nbsp; Proposition 65 information Revives wood and composite decks, railings, porches and boat docks, also great for concrete pool decks, patios and sidewalks100% acrylic solid color coating Resists cracking and peeling and conceals splinters and cracks up to 1/4 in. Provides a durable, mildew resistant finish Covers up to 75 sq. ft. in 2 coats per gallon Creates a textured, slip-resistant finish For best results, prepare with the appropriate BEHR product for your wood or concrete surface Actual paint colors may vary from on-screen and printer representations Colors available to be tinted in most stores Online Price includes Paint Care fee in the following states: CA, CO, CT, ME, MN, OR, RI, VT\n","Simple regex finished in  11.033880949020386 seconds\n","Measurement unit correction...\n","search term...\n","product_title...\n","brand...\n","product_description...\n","Measurement unit correction finished in  11.907288312911987 seconds\n","Removing stopwords...\n","The stopwords are:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","search_term...\n","product_title...\n","brand...\n","product_description...\n","behr premium textured deckover innovative solid color coating . bring old , weathered wood concrete back life . advanced 100 % acrylic resin formula creates durable coating tired worn deck , rejuvenating whole new look . best results , sure properly prepare surface using applicable behr products displayed . california residents : see & nbsp ; proposition 65 information revives wood composite decks , railings , porches boat docks , also great concrete pool decks , patios sidewalks100 % acrylic solid color coating resists cracking peeling conceals splinters cracks 1/4 . provides durable , mildew resistant finish covers 75 sq.ft . . 2 coats per gal . creates textured , slip-resistant finish best results , prepare appropriate behr product wood concrete surface actual paint colors may vary on-screen printer representations colors available tinted stores online price includes paint care fee following states : ca , co , ct , , mn , , ri , vt\n","Stopwords removal finished in  285.97128677368164 seconds\n","----------------------------------------------------\n","Removing punctuations...\n","The punctuations are:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","search_term...\n","product_title...\n","brand...\n","product_description...\n","behr premium textured deckover innovative solid color coating  bring old  weathered wood concrete back life  advanced 100  acrylic resin formula creates durable coating tired worn deck  rejuvenating whole new look  best results  sure properly prepare surface using applicable behr products displayed  california residents  see  nbsp  proposition 65 information revives wood composite decks  railings  porches boat docks  also great concrete pool decks  patios sidewalks100  acrylic solid color coating resists cracking peeling conceals splinters cracks 14  provides durable  mildew resistant finish covers 75 sqft   2 coats per gal  creates textured  slipresistant finish best results  prepare appropriate behr product wood concrete surface actual paint colors may vary onscreen printer representations colors available tinted stores online price includes paint care fee following states  ca  co  ct   mn   ri  vt\n","Punctuation removal finished in  1.8571646213531494 seconds\n","----------------------------------------------------\n","Spell correcting...\n","Spell correction finished in  0.14734625816345215 seconds\n","----------------------------------------------------\n","Singularizing...\n","search_term...\n","product_title...\n","product_description...\n","behr premium textured deckover innovative solid color coating bring old weathered wood concrete back life advanced 100 acrylic resin formula create durable coating tired worn deck rejuvenating whole new look best result sure properly prepare surface using applicable behr product displayed californium resident see nbsp proposition 65 information revife wood composite deck railing porch boat dock also great concrete pool deck patio sidewalks100 acrylic solid color coating resist cracking peeling conceal splinter crack 14 provide durable mildew resistant finish cover 75 sqft 2 coat per gal create textured slipresistant finish best result prepare appropriate behr product wood concrete surface actual paint color may vary onscreen printer representation color available tinted store online price include paint care fee following state ca co ct mn ri vt\n","Singularizing finished in  555.2434718608856 seconds\n","Performing simple stemming...\n","search_term...\n","product_title...\n","brand...\n","product_description...\n","behr premium textur deckov innov solid color coat bring old weather wood concret back life advanc 100 acryl resin formula creat durabl coat tire worn deck rejuven whole new look best result sure proper prepar surfac use applic behr product display californium resid see nbsp proposit 65 inform revif wood composit deck rail porch boat dock also great concret pool deck patio sidewalks100 acryl solid color coat resist crack peel conceal splinter crack 14 provid durabl mildew resist finish cover 75 sqft 2 coat per gal creat textur slipresist finish best result prepar appropri behr product wood concret surfac actual paint color may vari onscreen printer represent color avail tint store onlin price includ paint care fee follow state ca co ct mn ri vt\n","Simple stemming finished in  305.7413284778595 seconds\n","\n"," The shape of train_test with products_description is:  (240760, 7)\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 7 columns):\n"," #   Column               Non-Null Count   Dtype \n","---  ------               --------------   ----- \n"," 0   id                   240760 non-null  int64 \n"," 1   product_uid          240760 non-null  int64 \n"," 2   product_title        240760 non-null  object\n"," 3   search_term          240760 non-null  object\n"," 4   relevance            240760 non-null  object\n"," 5   brand                240760 non-null  object\n"," 6   product_description  240760 non-null  object\n","dtypes: int64(2), object(5)\n","memory usage: 14.7+ MB\n","None\n","creating len columns...\n","Length calculation finished in  1.7150254249572754 seconds\n","creating combination of columns...\n","creating common_words title/description...\n","Common words calculation finished in  2.861701011657715 seconds\n","creating ratio columns...\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 20 columns):\n"," #   Column                       Non-Null Count   Dtype  \n","---  ------                       --------------   -----  \n"," 0   id                           240760 non-null  int64  \n"," 1   product_uid                  240760 non-null  int64  \n"," 2   product_title                240760 non-null  object \n"," 3   search_term                  240760 non-null  object \n"," 4   relevance                    240760 non-null  object \n"," 5   brand                        240760 non-null  object \n"," 6   product_description          240760 non-null  object \n"," 7   len_of_query                 240760 non-null  int64  \n"," 8   len_of_title                 240760 non-null  int64  \n"," 9   len_of_description           240760 non-null  int64  \n"," 10  len_of_brand                 240760 non-null  int64  \n"," 11  product_all_information      240760 non-null  object \n"," 12  brand_helper                 240760 non-null  object \n"," 13  common_words_in_title        240760 non-null  int64  \n"," 14  common_words_in_description  240760 non-null  int64  \n"," 15  common_words_in_brand        240760 non-null  int64  \n"," 16  ratio_title                  240706 non-null  float64\n"," 17  ratio_description            240706 non-null  float64\n"," 18  query_title_len_prop         240706 non-null  float64\n"," 19  query_desc_len_prop          240706 non-null  float64\n","dtypes: float64(4), int64(9), object(7)\n","memory usage: 38.6+ MB\n","None\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 20 columns):\n"," #   Column                       Non-Null Count   Dtype  \n","---  ------                       --------------   -----  \n"," 0   id                           240760 non-null  int64  \n"," 1   product_uid                  240760 non-null  int64  \n"," 2   product_title                240760 non-null  object \n"," 3   search_term                  240760 non-null  object \n"," 4   relevance                    240760 non-null  object \n"," 5   brand                        240760 non-null  object \n"," 6   product_description          240760 non-null  object \n"," 7   len_of_query                 240760 non-null  int64  \n"," 8   len_of_title                 240760 non-null  int64  \n"," 9   len_of_description           240760 non-null  int64  \n"," 10  len_of_brand                 240760 non-null  int64  \n"," 11  product_all_information      240760 non-null  object \n"," 12  brand_helper                 240760 non-null  object \n"," 13  common_words_in_title        240760 non-null  int64  \n"," 14  common_words_in_description  240760 non-null  int64  \n"," 15  common_words_in_brand        240760 non-null  int64  \n"," 16  ratio_title                  240760 non-null  float64\n"," 17  ratio_description            240760 non-null  float64\n"," 18  query_title_len_prop         240760 non-null  float64\n"," 19  query_desc_len_prop          240760 non-null  float64\n","dtypes: float64(4), int64(9), object(7)\n","memory usage: 38.6+ MB\n","None\n","Ratio calculation finished in  0.6162803173065186 seconds\n","creating jacard columns ....\n","Jaccard calculation finished in  7.352694034576416 seconds\n","creating ngrams columns ....\n","ngrams calculation finished in  11.482855319976807 seconds\n","   id  product_uid  ... ngrams_search_and_description ngrams_search_and_brand\n","0   2       100001  ...                             0                       0\n","1   3       100001  ...                             0                       0\n","\n","[2 rows x 26 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 240760 entries, 0 to 240759\n","Data columns (total 19 columns):\n"," #   Column                          Non-Null Count   Dtype  \n","---  ------                          --------------   -----  \n"," 0   id                              240760 non-null  int64  \n"," 1   relevance                       240760 non-null  object \n"," 2   len_of_query                    240760 non-null  int64  \n"," 3   len_of_title                    240760 non-null  int64  \n"," 4   len_of_description              240760 non-null  int64  \n"," 5   len_of_brand                    240760 non-null  int64  \n"," 6   common_words_in_title           240760 non-null  int64  \n"," 7   common_words_in_description     240760 non-null  int64  \n"," 8   common_words_in_brand           240760 non-null  int64  \n"," 9   ratio_title                     240760 non-null  float64\n"," 10  ratio_description               240760 non-null  float64\n"," 11  query_title_len_prop            240760 non-null  float64\n"," 12  query_desc_len_prop             240760 non-null  float64\n"," 13  jaccard_search_and_title        240760 non-null  float64\n"," 14  jaccard_search_and_description  240760 non-null  float64\n"," 15  jaccard_search_and_brand        240760 non-null  float64\n"," 16  ngrams_search_and_title         240760 non-null  int64  \n"," 17  ngrams_search_and_description   240760 non-null  int64  \n"," 18  ngrams_search_and_brand         240760 non-null  int64  \n","dtypes: float64(7), int64(11), object(1)\n","memory usage: 36.7+ MB\n","None\n","(74067, 19)\n","   id relevance  ...  ngrams_search_and_description  ngrams_search_and_brand\n","0   2         3  ...                              0                        0\n","1   3       2.5  ...                              0                        0\n","2   9         3  ...                              0                        0\n","3  16      2.33  ...                              0                        0\n","4  17      2.67  ...                              1                        1\n","\n","[5 rows x 19 columns]\n","(166693, 19)\n","(166693,)\n","       id relevance  ...  ngrams_search_and_description  ngrams_search_and_brand\n","74067   1   unknown  ...                              0                        0\n","74068   4   unknown  ...                              0                        0\n","74069   5   unknown  ...                              0                        0\n","74070   6   unknown  ...                              0                        0\n","74071   7   unknown  ...                              0                        0\n","\n","[5 rows x 19 columns]\n","X_train shape  (51846, 17)\n","X test shape  (22221, 17)\n","Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    8.0s\n","[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.1s\n","[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   14.6s\n","[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   14.9s\n","[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   21.3s\n","[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   21.5s\n","[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   28.0s\n","[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   28.1s\n","[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   34.6s\n","[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   34.8s\n","[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   41.2s\n","[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   41.4s\n","[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   47.8s\n","[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   48.0s\n","[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   54.5s\n","[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   54.7s\n","[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.0min\n","[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.0min\n","[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  1.1min\n","[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  1.1min\n","[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.2min\n","[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  1.2min\n","[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  1.3min\n","[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.4min\n","[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.5min\n","[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.5min\n","[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  1.6min\n","[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.6min\n","[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.7min\n","[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.7min\n","[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  1.8min\n","[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  1.8min\n","[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.9min\n","[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.9min\n","[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  2.0min\n","[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  2.0min\n","[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.1min\n","[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  2.1min\n","[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  2.2min\n","[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  2.2min\n","[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  2.3min\n","[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.3min\n","[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:  2.4min\n","[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:  2.5min\n","[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.6min\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.6min\n","[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:  2.7min\n","[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  2.7min\n","[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:  2.8min\n","[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:  2.8min\n","[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:  2.9min\n","[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:  2.9min\n","[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  3.0min\n","[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:  3.0min\n","[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:  3.1min\n","[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.1min\n","[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  3.2min\n","[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:  3.2min\n","[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:  3.3min\n","[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:  3.3min\n","[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  3.5min\n","[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:  3.5min\n","[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:  3.6min\n","[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  3.6min\n","[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:  3.7min\n","[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:  3.7min\n","[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:  3.9min\n","[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  3.9min\n","[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  4.0min\n","[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:  4.0min\n","[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:  4.1min\n","[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:  4.2min\n","[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:  4.3min\n","[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  4.3min\n","[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:  4.4min\n","[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:  4.4min\n","[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  4.6min\n","[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:  4.6min\n","[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:  4.7min\n","[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:  4.7min\n","[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  4.8min\n","[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.8min\n","[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:  5.0min\n","[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:  5.0min\n","[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:  5.1min\n","[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:  5.1min\n","[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:  5.2min\n","[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:  5.3min\n","[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  5.4min\n","[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  5.4min\n","[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:  5.5min\n","[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:  5.5min\n","[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:  5.6min\n","[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  5.7min\n","[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:  5.8min\n","[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:  5.8min\n","[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  5.9min\n","[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:  5.9min\n","[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:  6.0min\n","[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:  6.1min\n","[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:  6.2min\n","[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:  6.2min\n","[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:  6.3min\n","[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  6.3min\n","[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  6.5min\n","[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:  6.5min\n","[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:  6.6min\n","[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:  6.6min\n","[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  6.7min\n","[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:  6.7min\n","[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:  6.9min\n","[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  6.9min\n","[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:  7.0min\n","[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:  7.0min\n","[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:  7.1min\n","[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:  7.1min\n","[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:  7.3min\n","[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:  7.3min\n","[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:  7.4min\n","[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  7.4min\n","[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  7.6min\n","[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:  7.6min\n","[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:  7.7min\n","[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  7.7min\n","[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:  7.9min\n","[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:  7.9min\n","[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:  8.1min\n","[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:  8.1min\n","[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  8.2min\n","[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  8.3min\n","[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:  8.4min\n","[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:  8.4min\n","[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:  8.6min\n","[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:  8.6min\n","[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:  8.7min\n","[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:  8.8min\n","[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  8.9min\n","[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  8.9min\n","[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:  9.1min\n","[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:  9.1min\n","[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:  9.2min\n","[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:  9.2min\n","[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:  9.4min\n","[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:  9.4min\n","[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:  9.6min\n","[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  9.6min\n","[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:  9.7min\n","[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:  9.7min\n","[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  9.9min\n","[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  9.9min\n","[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed: 10.1min\n","[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed: 10.1min\n","[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed: 10.2min\n","[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 10.2min\n","[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed: 10.4min\n","[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed: 10.4min\n","[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed: 10.5min\n","[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 10.5min\n","[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed: 10.7min\n","[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed: 10.7min\n","[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed: 10.9min\n","[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed: 10.9min\n","[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed: 11.0min\n","[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed: 11.0min\n","[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 11.2min\n","[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed: 11.2min\n","[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed: 11.3min\n","[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 11.3min\n","[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed: 11.5min\n","[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed: 11.5min\n","[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed: 11.7min\n","[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed: 11.7min\n","[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 11.8min\n","[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed: 11.8min\n","[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed: 12.0min\n","[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 12.0min\n","[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 12.1min\n","[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed: 12.1min\n","[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed: 12.3min\n","[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed: 12.3min\n","[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed: 12.4min\n","[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed: 12.4min\n","[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed: 12.5min\n","[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 12.5min\n","[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed: 12.6min\n","[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed: 12.6min\n","[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed: 12.7min\n","[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed: 12.8min\n","[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed: 12.9min\n","[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed: 12.9min\n","[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed: 13.0min\n","[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 13.0min\n","[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed: 13.1min\n","[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed: 13.1min\n","[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed: 13.2min\n","[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 13.2min\n","[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed: 13.3min\n","[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed: 13.3min\n","[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed: 13.4min\n","[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed: 13.4min\n","[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed: 13.5min\n","[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed: 13.5min\n","[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed: 13.6min\n","[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed: 13.7min\n","[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 13.8min\n","[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed: 13.8min\n","[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed: 13.9min\n","[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed: 13.9min\n","[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed: 14.0min\n","[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed: 14.0min\n","[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed: 14.1min\n","[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed: 14.1min\n","[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 14.2min\n","[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed: 14.2min\n","[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed: 14.3min\n","[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed: 14.3min\n","[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 14.4min\n","[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed: 14.4min\n","[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed: 14.5min\n","[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed: 14.5min\n","[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed: 14.6min\n","[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed: 14.6min\n","[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed: 14.8min\n","[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed: 14.8min\n","[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed: 14.9min\n","[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 14.9min\n","[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed: 15.0min\n","[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed: 15.0min\n","[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed: 15.1min\n","[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed: 15.1min\n","[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed: 15.2min\n","[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed: 15.2min\n","[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed: 15.3min\n","[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 15.3min\n","[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed: 15.4min\n","[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed: 15.4min\n","[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed: 15.5min\n","[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 15.5min\n","[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed: 15.6min\n","[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed: 15.6min\n","[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed: 15.8min\n","[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed: 15.8min\n","[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed: 15.9min\n","[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed: 15.9min\n","[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed: 16.0min\n","[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed: 16.0min\n","[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed: 16.2min\n","[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed: 16.2min\n","[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed: 16.3min\n","[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed: 16.3min\n","[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed: 16.4min\n","[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed: 16.5min\n","[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed: 16.6min\n","[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed: 16.6min\n","[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed: 16.7min\n","[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 16.7min\n","[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 16.9min\n","[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed: 16.9min\n","[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed: 17.0min\n","[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed: 17.0min\n","[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed: 17.1min\n","[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed: 17.1min\n","[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed: 17.3min\n","[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 17.3min\n","[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed: 17.4min\n","[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed: 17.4min\n","[Parallel(n_jobs=-1)]: Done 267 tasks      | elapsed: 17.5min\n","[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed: 17.5min\n","[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed: 17.7min\n","[Parallel(n_jobs=-1)]: Done 270 tasks      | elapsed: 17.7min\n","[Parallel(n_jobs=-1)]: Done 271 tasks      | elapsed: 17.8min\n","[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 17.8min\n","[Parallel(n_jobs=-1)]: Done 273 tasks      | elapsed: 17.9min\n","[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed: 18.0min\n","[Parallel(n_jobs=-1)]: Done 275 tasks      | elapsed: 18.1min\n","[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed: 18.1min\n","[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed: 18.2min\n","[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed: 18.2min\n","[Parallel(n_jobs=-1)]: Done 279 tasks      | elapsed: 18.3min\n","[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 18.4min\n","[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed: 18.5min\n","[Parallel(n_jobs=-1)]: Done 282 tasks      | elapsed: 18.5min\n","[Parallel(n_jobs=-1)]: Done 283 tasks      | elapsed: 18.6min\n","[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 18.6min\n","[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed: 18.8min\n","[Parallel(n_jobs=-1)]: Done 286 tasks      | elapsed: 18.8min\n","[Parallel(n_jobs=-1)]: Done 287 tasks      | elapsed: 19.0min\n","[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed: 19.0min\n","[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 19.1min\n","[Parallel(n_jobs=-1)]: Done 290 tasks      | elapsed: 19.1min\n","[Parallel(n_jobs=-1)]: Done 291 tasks      | elapsed: 19.2min\n","[Parallel(n_jobs=-1)]: Done 292 tasks      | elapsed: 19.2min\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tvr0gBrd7w26","colab_type":"code","colab":{}},"source":["drive.flush_and_unmount()\n","print('All changes made in this colab session should now be visible in Drive.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZcI-sVyxtF8u","colab":{}},"source":["# rf = RandomForestRegressor(random_state=randomSeed)\n","\n","# # Set the parameter values for GridSearch\n","# param_grid = {'max_samples': [0.1, 0.2, 0.4],\n","#               'n_estimators': [100, 200, 300],\n","#               'min_samples_split': [2, 4, 6]\n","#              }\n","\n","# model = GridSearchCV(estimator= rf,\n","#                     param_grid= param_grid,\n","#                     n_jobs= -1,\n","#                     cv= 10, \n","#                     verbose= 20,\n","#                     scoring= RMSE\n","#                     )\n","\n","# model.fit(X_train, y_train)\n","# y_pred = model.predict(X_test)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)\n","\n","# # Gradient Boosting regressor. It has not been used in conjunction to Bagging regressor\n","# gb = GradientBoostingRegressor(random_state=randomSeed)\n","# gb.fit(X_train, y_train)\n","# y_pred = gb.predict(X_test)\n","\n","# model = GridSearchCV(estimator= clf,\n","#                                 param_grid= param_grid,\n","#                                 n_jobs= -1,\n","#                                 cv= 2, \n","#                                 verbose= 20,\n","#                                 scoring= RMSE)\n","# model.fit(X_train, y_train)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sNVwwJXbtbZG"},"source":["3. Bagging Regressor to improve Random Forest"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aud7y-wttePq","colab":{}},"source":["# clf = BaggingRegressor(base_estimator= rf, random_state=randomSeed)\n","# clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# # Set the parameter values for GridSearch\n","# param_grid = {'max_samples': [0.1, 0.2],\n","#               'n_estimators': [10, 100, 150],\n","#               'min_samples_split': [2, 4, 6]\n","#              }\n","\n","# model = GridSearchCV(\n","#     estimator= rf,\n","#     param_grid= param_grid,\n","#     n_jobs= -1,\n","#     cv= 10, \n","#     verbose= 20,\n","#     scoring= RMSE\n","# )\n","\n","# model.fit(X_train, y_train)\n","# y_pred = model.predict(X_test)\n","# print(\"RF: Best parameters found by grid search:\", model.best_params_)\n","# print(\"Random Forest's best CV score: %.4f\" % model.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzNbcBbIqbV8","colab_type":"code","colab":{}},"source":["# descr = pd.read_csv('/content/drive/My Drive/Colab Notebooks/product_descriptions.csv', header=0)\n","# pd.options.display.max_rows = 100\n","# pd.set_option('max_colwidth', 9000)\n","# print(descr[descr['product_description'].str.contains(\"oz\")]['product_description'])\n","\n","# # descr['product_description'].apply(lambda x: ngrams(x.lower().split(), 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"am0UeNYZ-aM6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}